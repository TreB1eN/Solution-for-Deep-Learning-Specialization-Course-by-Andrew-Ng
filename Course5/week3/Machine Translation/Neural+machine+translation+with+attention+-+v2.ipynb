{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Welcome to your first programming assignment for this week! \n",
    "\n",
    "You will build a Neural Machine Translation (NMT) model to translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\"). You will do this using an attention model, one of the most sophisticated sequence to sequence models. \n",
    "\n",
    "This notebook was produced together with NVIDIA's Deep Learning Institute. \n",
    "\n",
    "Let's load all the packages you will need for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Translating human readable dates into machine readable dates\n",
    "\n",
    "The model you will build here could be used to translate from one language to another, such as translating from English to Hindi. However, language translation requires massive datasets and usually takes days of training on GPUs. To give you a place to experiment with these models even without using massive datasets, we will instead use a simpler \"date translation\" task. \n",
    "\n",
    "The network will input a date written in a variety of possible formats (*e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"*) and translate them into standardized, machine readable dates (*e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*). We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD. \n",
    "\n",
    "\n",
    "\n",
    "<!-- \n",
    "Take a look at [nmt_utils.py](./nmt_utils.py) to see all the formatting. Count and figure out how the formats work, you will need this knowledge later. !--> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Dataset\n",
    "\n",
    "We will train the model on a dataset of 10000 human readable dates and their equivalent, standardized, machine readable dates. Let's run the following cells to load the dataset and print some examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 19686.11it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('27 july 1983', '1983-07-27'),\n",
       " ('thursday june 6 1974', '1974-06-06'),\n",
       " ('tuesday january 3 1995', '1995-01-03'),\n",
       " ('7/6/91', '1991-07-06'),\n",
       " ('saturday june 21 1980', '1980-06-21'),\n",
       " ('sunday december 26 2010', '2010-12-26'),\n",
       " ('24 nov 2008', '2008-11-24'),\n",
       " ('10 06 04', '2004-06-10'),\n",
       " ('monday may 23 2011', '2011-05-23'),\n",
       " ('wednesday april 8 2015', '2015-04-08')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've loaded:\n",
    "- `dataset`: a list of tuples of (human readable date, machine readable date)\n",
    "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index \n",
    "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. These indices are not necessarily consistent with `human_vocab`. \n",
    "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. \n",
    "\n",
    "Let's preprocess the data and map the raw text data into the index values. We will also use Tx=30 (which we assume is the maximum length of the human readable date; if we get a longer input, we would have to truncate it) and Ty=10 (since \"YYYY-MM-DD\" is 10 characters long). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have:\n",
    "- `X`: a processed version of the human readable dates in the training set, where each character is replaced by an index mapped to the character via `human_vocab`. Each date is further padded to $T_x$ values with a special character (< pad >). `X.shape = (m, Tx)`\n",
    "- `Y`: a processed version of the machine readable dates in the training set, where each character is replaced by the index it is mapped to in `machine_vocab`. You should have `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`, the \"1\" entry's index is mapped to the character thanks to `human_vocab`. `Xoh.shape = (m, Tx, len(human_vocab))`\n",
    "- `Yoh`: one-hot version of `Y`, the \"1\" entry's index is mapped to the character thanks to `machine_vocab`. `Yoh.shape = (m, Tx, len(machine_vocab))`. Here, `len(machine_vocab) = 11` since there are 11 characters ('-' as well as 0-9). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also look at some examples of preprocessed training examples. Feel free to play with `index` in the cell below to navigate the dataset and see how source/target dates are preprocessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 27 july 1983\n",
      "Target date: 1983-07-27\n",
      "\n",
      "Source after preprocessing (indices): [ 5 10  0 22 31 23 34  0  4 12 11  6 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10  9  4  0  1  8  0  3  8]\n",
      "\n",
      "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Neural machine translation with attention\n",
    "\n",
    "If you had to translate a book's paragraph from French to English, you would not read the whole paragraph, then close the book and translate. Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. \n",
    "\n",
    "The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. \n",
    "\n",
    "\n",
    "### 2.1 - Attention mechanism\n",
    "\n",
    "In this part, you will implement the attention mechanism presented in the lecture videos. Here is a figure to remind you how the model works. The diagram on the left shows the attention model. The diagram on the right shows what one \"Attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$, which are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here are some properties of the model that you may notice: \n",
    "\n",
    "- There are two separate LSTMs in this model (see diagram on the left). Because the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism, we will call it *pre-attention* Bi-LSTM. The LSTM at the top of the diagram comes *after* the attention mechanism, so we will call it the *post-attention* LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps. \n",
    "\n",
    "- The post-attention LSTM passes $s^{\\langle t \\rangle}, c^{\\langle t \\rangle}$ from one time step to the next. In the lecture videos, we were using only a basic RNN for the post-activation sequence model, so the state captured by the RNN output activations $s^{\\langle t\\rangle}$. But since we are using an LSTM here, the LSTM has both the output activation $s^{\\langle t\\rangle}$ and the hidden cell state $c^{\\langle t\\rangle}$. However, unlike previous text generation examples (such as Dinosaurus in week 1), in this model the post-activation LSTM at time $t$ does will not take the specific generated $y^{\\langle t-1 \\rangle}$ as input; it only takes $s^{\\langle t\\rangle}$ and $c^{\\langle t\\rangle}$ as input. We have designed the model this way, because (unlike language generation where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date. \n",
    "\n",
    "- We use $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}; \\overleftarrow{a}^{\\langle t \\rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM. \n",
    "\n",
    "- The diagram on the right uses a `RepeatVector` node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times, and then `Concatenation` to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ to compute $e^{\\langle t, t'}$, which is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$. We'll explain how to use `RepeatVector` and `Concatenation` in Keras below. \n",
    "\n",
    "Lets implement this model. You will start by implementing two functions: `one_step_attention()` and `model()`.\n",
    "\n",
    "**1) `one_step_attention()`**: At step $t$, given all the hidden states of the Bi-LSTM ($[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$) and the previous hidden state of the second LSTM ($s^{<t-1>}$), `one_step_attention()` will compute the attention weights ($[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$) and output the context vector (see Figure  1 (right) for details):\n",
    "$$context^{<t>} = \\sum_{t' = 0}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "Note that we are denoting the attention in this notebook $context^{\\langle t \\rangle}$. In the lecture videos, the context was denoted $c^{\\langle t \\rangle}$, but here we are calling it $context^{\\langle t \\rangle}$ to avoid confusion with the (post-attention) LSTM's internal memory cell variable, which is sometimes also denoted $c^{\\langle t \\rangle}$. \n",
    "  \n",
    "**2) `model()`**: Implements the entire model. It first runs the input through a Bi-LSTM to get back $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$. Then, it calls `one_step_attention()` $T_y$ times (`for` loop). At each iteration of this loop, it gives the computed context vector $c^{<t>}$ to the second LSTM, and runs the output of the LSTM through a dense layer with softmax activation to generate a prediction $\\hat{y}^{<t>}$. \n",
    "\n",
    "\n",
    "\n",
    "**Exercise**: Implement `one_step_attention()`. The function `model()` will call the layers in `one_step_attention()` $T_y$ using a for-loop, and it is important that all $T_y$ copies have the same weights. I.e., it should not re-initiaiize the weights every time. In other words, all $T_y$ steps should have shared weights. Here's how you can implement layers with shareable weights in Keras:\n",
    "1. Define the layer objects (as global variables for examples).\n",
    "2. Call these objects when propagating the input.\n",
    "\n",
    "We have defined the layers you need as global variables. Please run the following cells to create them. Please check the Keras documentation to make sure you understand what these layers are: [RepeatVector()](https://keras.io/layers/core/#repeatvector), [Concatenate()](https://keras.io/layers/merge/#concatenate), [Dense()](https://keras.io/layers/core/#dense), [Activation()](https://keras.io/layers/core/#activation), [Dot()](https://keras.io/layers/merge/#dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use these layers to implement `one_step_attention()`. In order to propagate a Keras tensor object X through one of these layers, use `layer(X)` (or `layer([X,Y])` if it requires multiple inputs.), e.g. `densor(X)` will propagate X through the `Dense(1)` layer defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_step_attention\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([s_prev,a])\n",
    "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
    "    e = densor(concat)\n",
    "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(e)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas,a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to check the expected output of `one_step_attention()` after you've coded the `model()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement `model()` as explained in figure 2 and the text above. Again, we have defined global layers that will share weights to be used in `model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_a = 64\n",
    "n_s = 128\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use these layers $T_y$ times in a `for` loop to generate the outputs, and their parameters will not be reinitialized. You will have to carry out the following steps: \n",
    "\n",
    "1. Propagate the input into a [Bidirectional](https://keras.io/layers/wrappers/#bidirectional) [LSTM](https://keras.io/layers/recurrent/#lstm)\n",
    "2. Iterate for $t = 0, \\dots, T_y-1$: \n",
    "    1. Call `one_step_attention()` on $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$ and $s^{<t-1>}$ to get the context vector $context^{<t>}$.\n",
    "    2. Give $context^{<t>}$ to the post-attention LSTM cell. Remember pass in the previous hidden-state $s^{\\langle t-1\\rangle}$ and cell-states $c^{\\langle t-1\\rangle}$ of this LSTM using `initial_state= [previous hidden state, previous cell state]`. Get back the new hidden state $s^{<t>}$ and the new cell state $c^{<t>}$.\n",
    "    3. Apply a softmax layer to $s^{<t>}$, get the output. \n",
    "    4. Save the output by adding it to the list of outputs.\n",
    "\n",
    "3. Create your Keras model instance, it should have three inputs (\"inputs\", $s^{<0>}$ and $c^{<0>}$) and output the list of \"outputs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = Bidirectional(LSTM(n_a,return_sequences=True))(X)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a,s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context,initial_state=[s,c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model([X,s0,c0],outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a summary of the model to check if it matches the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 10, 64, 128, 37, 11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "s0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 30, 128)      0           s0[0][0]                         \n",
      "                                                                 lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 lstm_2[2][0]                     \n",
      "                                                                 lstm_2[3][0]                     \n",
      "                                                                 lstm_2[4][0]                     \n",
      "                                                                 lstm_2[5][0]                     \n",
      "                                                                 lstm_2[6][0]                     \n",
      "                                                                 lstm_2[7][0]                     \n",
      "                                                                 lstm_2[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 128)      52224       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 30, 256)      0           repeat_vector_2[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[9][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 30, 1)        257         concatenate_2[0][0]              \n",
      "                                                                 concatenate_2[1][0]              \n",
      "                                                                 concatenate_2[2][0]              \n",
      "                                                                 concatenate_2[3][0]              \n",
      "                                                                 concatenate_2[4][0]              \n",
      "                                                                 concatenate_2[5][0]              \n",
      "                                                                 concatenate_2[6][0]              \n",
      "                                                                 concatenate_2[7][0]              \n",
      "                                                                 concatenate_2[8][0]              \n",
      "                                                                 concatenate_2[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_3[0][0]                    \n",
      "                                                                 dense_3[1][0]                    \n",
      "                                                                 dense_3[2][0]                    \n",
      "                                                                 dense_3[3][0]                    \n",
      "                                                                 dense_3[4][0]                    \n",
      "                                                                 dense_3[5][0]                    \n",
      "                                                                 dense_3[6][0]                    \n",
      "                                                                 dense_3[7][0]                    \n",
      "                                                                 dense_3[8][0]                    \n",
      "                                                                 dense_3[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 128), (None, 131584      dot_2[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_2[1][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "                                                                 dot_2[2][0]                      \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 lstm_2[1][2]                     \n",
      "                                                                 dot_2[3][0]                      \n",
      "                                                                 lstm_2[2][0]                     \n",
      "                                                                 lstm_2[2][2]                     \n",
      "                                                                 dot_2[4][0]                      \n",
      "                                                                 lstm_2[3][0]                     \n",
      "                                                                 lstm_2[3][2]                     \n",
      "                                                                 dot_2[5][0]                      \n",
      "                                                                 lstm_2[4][0]                     \n",
      "                                                                 lstm_2[4][2]                     \n",
      "                                                                 dot_2[6][0]                      \n",
      "                                                                 lstm_2[5][0]                     \n",
      "                                                                 lstm_2[5][2]                     \n",
      "                                                                 dot_2[7][0]                      \n",
      "                                                                 lstm_2[6][0]                     \n",
      "                                                                 lstm_2[6][2]                     \n",
      "                                                                 dot_2[8][0]                      \n",
      "                                                                 lstm_2[7][0]                     \n",
      "                                                                 lstm_2[7][2]                     \n",
      "                                                                 dot_2[9][0]                      \n",
      "                                                                 lstm_2[8][0]                     \n",
      "                                                                 lstm_2[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 11)           1419        lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 lstm_2[2][0]                     \n",
      "                                                                 lstm_2[3][0]                     \n",
      "                                                                 lstm_2[4][0]                     \n",
      "                                                                 lstm_2[5][0]                     \n",
      "                                                                 lstm_2[6][0]                     \n",
      "                                                                 lstm_2[7][0]                     \n",
      "                                                                 lstm_2[8][0]                     \n",
      "                                                                 lstm_2[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 185,484\n",
      "Trainable params: 185,484\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "Here is the summary you should see\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Total params:**\n",
    "        </td>\n",
    "        <td>\n",
    "         185,484\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **Trainable params:**\n",
    "        </td>\n",
    "        <td>\n",
    "         185,484\n",
    "        </td>\n",
    "    </tr>\n",
    "            <tr>\n",
    "        <td>\n",
    "            **Non-trainable params:**\n",
    "        </td>\n",
    "        <td>\n",
    "         0\n",
    "        </td>\n",
    "    </tr>\n",
    "                    <tr>\n",
    "        <td>\n",
    "            **bidirectional_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 128)  \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **repeat_vector_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 128)  \n",
    "        </td>\n",
    "    </tr>\n",
    "                <tr>\n",
    "        <td>\n",
    "            **concatenate_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 256) \n",
    "        </td>\n",
    "    </tr>\n",
    "            <tr>\n",
    "        <td>\n",
    "            **attention_weights's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 1)  \n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **dot_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 1, 128) \n",
    "        </td>\n",
    "    </tr>\n",
    "           <tr>\n",
    "        <td>\n",
    "            **dense_2's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 11) \n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using `categorical_crossentropy` loss, a custom [Adam](https://keras.io/optimizers/#adam) [optimizer](https://keras.io/optimizers/#usage-of-optimizers) (`learning rate = 0.005`, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, `decay = 0.01`)  and `['accuracy']` metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses,metrics\n",
    "### START CODE HERE ### (≈2 lines)\n",
    "opt = Adam(lr=0.005,decay=0.01)\n",
    "model.compile(opt,losses.categorical_crossentropy,[metrics.categorical_accuracy])\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to define all your inputs and outputs to fit the model:\n",
    "- You already have X of shape $(m = 10000, T_x = 30)$ containing the training examples.\n",
    "- You need to create `s0` and `c0` to initialize your `post_activation_LSTM_cell` with 0s.\n",
    "- Given the `model()` you coded, you need the \"outputs\" to be a list of 11 elements of shape (m, T_y). So that: `outputs[i][0], ..., outputs[i][Ty]` represent the true labels (characters) corresponding to the $i^{th}$ training example (`X[i]`). More generally, `outputs[i][j]` is the true label of the $j^{th}$ character in the $i^{th}$ training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit the model and run it for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1200/10000 [==>...........................] - ETA: 14:50 - loss: 23.9340 - dense_4_loss_1: 2.3941 - dense_4_loss_2: 2.4101 - dense_4_loss_3: 2.4058 - dense_4_loss_4: 2.4010 - dense_4_loss_5: 2.3624 - dense_4_loss_6: 2.4097 - dense_4_loss_7: 2.3970 - dense_4_loss_8: 2.3556 - dense_4_loss_9: 2.3965 - dense_4_loss_10: 2.4018 - dense_4_categorical_accuracy_1: 0.1000 - dense_4_categorical_accuracy_2: 0.0000e+00 - dense_4_categorical_accuracy_3: 0.0300 - dense_4_categorical_accuracy_4: 0.0200 - dense_4_categorical_accuracy_5: 0.6800 - dense_4_categorical_accuracy_6: 0.0100 - dense_4_categorical_accuracy_7: 0.0700 - dense_4_categorical_accuracy_8: 0.5600 - dense_4_categorical_accuracy_9: 0.0500 - dense_4_categorical_accuracy_10: 0.010 - ETA: 7:30 - loss: 23.4956 - dense_4_loss_1: 2.3798 - dense_4_loss_2: 2.3896 - dense_4_loss_3: 2.3959 - dense_4_loss_4: 2.4224 - dense_4_loss_5: 2.2159 - dense_4_loss_6: 2.3268 - dense_4_loss_7: 2.4086 - dense_4_loss_8: 2.1720 - dense_4_loss_9: 2.3558 - dense_4_loss_10: 2.4288 - dense_4_categorical_accuracy_1: 0.0500 - dense_4_categorical_accuracy_2: 0.0000e+00 - dense_4_categorical_accuracy_3: 0.0150 - dense_4_categorical_accuracy_4: 0.0100 - dense_4_categorical_accuracy_5: 0.8400 - dense_4_categorical_accuracy_6: 0.0050 - dense_4_categorical_accuracy_7: 0.0350 - dense_4_categorical_accuracy_8: 0.7800 - dense_4_categorical_accuracy_9: 0.0250 - dense_4_categorical_accuracy_10: 0.005 - ETA: 5:04 - loss: 23.0524 - dense_4_loss_1: 2.3597 - dense_4_loss_2: 2.3756 - dense_4_loss_3: 2.4041 - dense_4_loss_4: 2.4738 - dense_4_loss_5: 1.9802 - dense_4_loss_6: 2.2265 - dense_4_loss_7: 2.4954 - dense_4_loss_8: 1.8572 - dense_4_loss_9: 2.3149 - dense_4_loss_10: 2.5650 - dense_4_categorical_accuracy_1: 0.0333 - dense_4_categorical_accuracy_2: 0.0000e+00 - dense_4_categorical_accuracy_3: 0.0100 - dense_4_categorical_accuracy_4: 0.0067 - dense_4_categorical_accuracy_5: 0.8933 - dense_4_categorical_accuracy_6: 0.0033 - dense_4_categorical_accuracy_7: 0.0233 - dense_4_categorical_accuracy_8: 0.8533 - dense_4_categorical_accuracy_9: 0.0167 - dense_4_categorical_accuracy_10: 0.00 - ETA: 3:50 - loss: 22.7718 - dense_4_loss_1: 2.3288 - dense_4_loss_2: 2.3533 - dense_4_loss_3: 2.4261 - dense_4_loss_4: 2.5422 - dense_4_loss_5: 1.8143 - dense_4_loss_6: 2.0817 - dense_4_loss_7: 2.5731 - dense_4_loss_8: 1.6749 - dense_4_loss_9: 2.2399 - dense_4_loss_10: 2.7374 - dense_4_categorical_accuracy_1: 0.0250 - dense_4_categorical_accuracy_2: 0.0000e+00 - dense_4_categorical_accuracy_3: 0.0075 - dense_4_categorical_accuracy_4: 0.0050 - dense_4_categorical_accuracy_5: 0.9200 - dense_4_categorical_accuracy_6: 0.0025 - dense_4_categorical_accuracy_7: 0.0175 - dense_4_categorical_accuracy_8: 0.8900 - dense_4_categorical_accuracy_9: 0.0125 - dense_4_categorical_accuracy_10: 0.00 - ETA: 3:06 - loss: 22.5646 - dense_4_loss_1: 2.3013 - dense_4_loss_2: 2.3241 - dense_4_loss_3: 2.4270 - dense_4_loss_4: 2.5625 - dense_4_loss_5: 1.7857 - dense_4_loss_6: 1.9847 - dense_4_loss_7: 2.5822 - dense_4_loss_8: 1.6735 - dense_4_loss_9: 2.1516 - dense_4_loss_10: 2.7720 - dense_4_categorical_accuracy_1: 0.0200 - dense_4_categorical_accuracy_2: 0.0000e+00 - dense_4_categorical_accuracy_3: 0.0120 - dense_4_categorical_accuracy_4: 0.0240 - dense_4_categorical_accuracy_5: 0.7380 - dense_4_categorical_accuracy_6: 0.1380 - dense_4_categorical_accuracy_7: 0.0320 - dense_4_categorical_accuracy_8: 0.7120 - dense_4_categorical_accuracy_9: 0.0640 - dense_4_categorical_accuracy_10: 0.03 - ETA: 2:37 - loss: 22.4318 - dense_4_loss_1: 2.2843 - dense_4_loss_2: 2.2883 - dense_4_loss_3: 2.4019 - dense_4_loss_4: 2.5808 - dense_4_loss_5: 1.7801 - dense_4_loss_6: 1.9338 - dense_4_loss_7: 2.5967 - dense_4_loss_8: 1.6925 - dense_4_loss_9: 2.0961 - dense_4_loss_10: 2.7771 - dense_4_categorical_accuracy_1: 0.0383 - dense_4_categorical_accuracy_2: 0.0000e+00 - dense_4_categorical_accuracy_3: 0.0350 - dense_4_categorical_accuracy_4: 0.0417 - dense_4_categorical_accuracy_5: 0.6150 - dense_4_categorical_accuracy_6: 0.1583 - dense_4_categorical_accuracy_7: 0.0600 - dense_4_categorical_accuracy_8: 0.5933 - dense_4_categorical_accuracy_9: 0.0933 - dense_4_categorical_accuracy_10: 0.04 - ETA: 2:16 - loss: 22.3327 - dense_4_loss_1: 2.2738 - dense_4_loss_2: 2.2582 - dense_4_loss_3: 2.3884 - dense_4_loss_4: 2.5989 - dense_4_loss_5: 1.7658 - dense_4_loss_6: 1.9059 - dense_4_loss_7: 2.6064 - dense_4_loss_8: 1.6929 - dense_4_loss_9: 2.0786 - dense_4_loss_10: 2.7639 - dense_4_categorical_accuracy_1: 0.0329 - dense_4_categorical_accuracy_2: 0.0000e+00 - dense_4_categorical_accuracy_3: 0.0300 - dense_4_categorical_accuracy_4: 0.0357 - dense_4_categorical_accuracy_5: 0.6700 - dense_4_categorical_accuracy_6: 0.1529 - dense_4_categorical_accuracy_7: 0.0514 - dense_4_categorical_accuracy_8: 0.5800 - dense_4_categorical_accuracy_9: 0.1029 - dense_4_categorical_accuracy_10: 0.04 - ETA: 2:00 - loss: 22.2289 - dense_4_loss_1: 2.2679 - dense_4_loss_2: 2.2290 - dense_4_loss_3: 2.3744 - dense_4_loss_4: 2.6137 - dense_4_loss_5: 1.7332 - dense_4_loss_6: 1.8846 - dense_4_loss_7: 2.6212 - dense_4_loss_8: 1.6685 - dense_4_loss_9: 2.0781 - dense_4_loss_10: 2.7585 - dense_4_categorical_accuracy_1: 0.0287 - dense_4_categorical_accuracy_2: 0.0000e+00 - dense_4_categorical_accuracy_3: 0.0263 - dense_4_categorical_accuracy_4: 0.0312 - dense_4_categorical_accuracy_5: 0.7113 - dense_4_categorical_accuracy_6: 0.1337 - dense_4_categorical_accuracy_7: 0.0450 - dense_4_categorical_accuracy_8: 0.6325 - dense_4_categorical_accuracy_9: 0.0900 - dense_4_categorical_accuracy_10: 0.04 - ETA: 1:47 - loss: 22.1106 - dense_4_loss_1: 2.2587 - dense_4_loss_2: 2.2074 - dense_4_loss_3: 2.3681 - dense_4_loss_4: 2.6396 - dense_4_loss_5: 1.6855 - dense_4_loss_6: 1.8696 - dense_4_loss_7: 2.6232 - dense_4_loss_8: 1.6261 - dense_4_loss_9: 2.0737 - dense_4_loss_10: 2.7587 - dense_4_categorical_accuracy_1: 0.0256 - dense_4_categorical_accuracy_2: 0.0000e+00 - dense_4_categorical_accuracy_3: 0.0233 - dense_4_categorical_accuracy_4: 0.0278 - dense_4_categorical_accuracy_5: 0.7433 - dense_4_categorical_accuracy_6: 0.1189 - dense_4_categorical_accuracy_7: 0.0400 - dense_4_categorical_accuracy_8: 0.6733 - dense_4_categorical_accuracy_9: 0.0800 - dense_4_categorical_accuracy_10: 0.03 - ETA: 1:38 - loss: 22.0307 - dense_4_loss_1: 2.2515 - dense_4_loss_2: 2.1843 - dense_4_loss_3: 2.3680 - dense_4_loss_4: 2.6666 - dense_4_loss_5: 1.6319 - dense_4_loss_6: 1.8491 - dense_4_loss_7: 2.6514 - dense_4_loss_8: 1.5777 - dense_4_loss_9: 2.0760 - dense_4_loss_10: 2.7742 - dense_4_categorical_accuracy_1: 0.0230 - dense_4_categorical_accuracy_2: 0.0000e+00 - dense_4_categorical_accuracy_3: 0.0210 - dense_4_categorical_accuracy_4: 0.0250 - dense_4_categorical_accuracy_5: 0.7690 - dense_4_categorical_accuracy_6: 0.1070 - dense_4_categorical_accuracy_7: 0.0360 - dense_4_categorical_accuracy_8: 0.7060 - dense_4_categorical_accuracy_9: 0.0720 - dense_4_categorical_accuracy_10: 0.03 - ETA: 1:29 - loss: 21.9599 - dense_4_loss_1: 2.2445 - dense_4_loss_2: 2.1628 - dense_4_loss_3: 2.3664 - dense_4_loss_4: 2.6858 - dense_4_loss_5: 1.5906 - dense_4_loss_6: 1.8273 - dense_4_loss_7: 2.6770 - dense_4_loss_8: 1.5439 - dense_4_loss_9: 2.0656 - dense_4_loss_10: 2.7959 - dense_4_categorical_accuracy_1: 0.0209 - dense_4_categorical_accuracy_2: 0.0000e+00 - dense_4_categorical_accuracy_3: 0.0191 - dense_4_categorical_accuracy_4: 0.0227 - dense_4_categorical_accuracy_5: 0.7900 - dense_4_categorical_accuracy_6: 0.0973 - dense_4_categorical_accuracy_7: 0.0327 - dense_4_categorical_accuracy_8: 0.7327 - dense_4_categorical_accuracy_9: 0.0655 - dense_4_categorical_accuracy_10: 0.03 - ETA: 1:23 - loss: 21.8784 - dense_4_loss_1: 2.2376 - dense_4_loss_2: 2.1425 - dense_4_loss_3: 2.3599 - dense_4_loss_4: 2.7098 - dense_4_loss_5: 1.5651 - dense_4_loss_6: 1.8136 - dense_4_loss_7: 2.6692 - dense_4_loss_8: 1.5289 - dense_4_loss_9: 2.0600 - dense_4_loss_10: 2.7918 - dense_4_categorical_accuracy_1: 0.0350 - dense_4_categorical_accuracy_2: 0.0000e+00 - dense_4_categorical_accuracy_3: 0.0175 - dense_4_categorical_accuracy_4: 0.0208 - dense_4_categorical_accuracy_5: 0.8075 - dense_4_categorical_accuracy_6: 0.0892 - dense_4_categorical_accuracy_7: 0.0300 - dense_4_categorical_accuracy_8: 0.7550 - dense_4_categorical_accuracy_9: 0.0600 - dense_4_categorical_accuracy_10: 0.0275\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2400/10000 [======>.......................] - ETA: 1:17 - loss: 21.7890 - dense_4_loss_1: 2.2281 - dense_4_loss_2: 2.1223 - dense_4_loss_3: 2.3574 - dense_4_loss_4: 2.7224 - dense_4_loss_5: 1.5493 - dense_4_loss_6: 1.8024 - dense_4_loss_7: 2.6561 - dense_4_loss_8: 1.5255 - dense_4_loss_9: 2.0501 - dense_4_loss_10: 2.7753 - dense_4_categorical_accuracy_1: 0.0838 - dense_4_categorical_accuracy_2: 0.0031 - dense_4_categorical_accuracy_3: 0.0162 - dense_4_categorical_accuracy_4: 0.0192 - dense_4_categorical_accuracy_5: 0.8223 - dense_4_categorical_accuracy_6: 0.0823 - dense_4_categorical_accuracy_7: 0.0277 - dense_4_categorical_accuracy_8: 0.7738 - dense_4_categorical_accuracy_9: 0.0554 - dense_4_categorical_accuracy_10: 0.0254   - ETA: 1:12 - loss: 21.7252 - dense_4_loss_1: 2.2205 - dense_4_loss_2: 2.0976 - dense_4_loss_3: 2.3491 - dense_4_loss_4: 2.7289 - dense_4_loss_5: 1.5370 - dense_4_loss_6: 1.7902 - dense_4_loss_7: 2.6583 - dense_4_loss_8: 1.5246 - dense_4_loss_9: 2.0453 - dense_4_loss_10: 2.7735 - dense_4_categorical_accuracy_1: 0.1221 - dense_4_categorical_accuracy_2: 0.0143 - dense_4_categorical_accuracy_3: 0.0150 - dense_4_categorical_accuracy_4: 0.0179 - dense_4_categorical_accuracy_5: 0.8350 - dense_4_categorical_accuracy_6: 0.0764 - dense_4_categorical_accuracy_7: 0.0257 - dense_4_categorical_accuracy_8: 0.7900 - dense_4_categorical_accuracy_9: 0.0600 - dense_4_categorical_accuracy_10: 0.03 - ETA: 1:08 - loss: 21.6416 - dense_4_loss_1: 2.2076 - dense_4_loss_2: 2.0655 - dense_4_loss_3: 2.3423 - dense_4_loss_4: 2.7421 - dense_4_loss_5: 1.5247 - dense_4_loss_6: 1.7746 - dense_4_loss_7: 2.6606 - dense_4_loss_8: 1.5191 - dense_4_loss_9: 2.0344 - dense_4_loss_10: 2.7705 - dense_4_categorical_accuracy_1: 0.1140 - dense_4_categorical_accuracy_2: 0.0613 - dense_4_categorical_accuracy_3: 0.0240 - dense_4_categorical_accuracy_4: 0.0220 - dense_4_categorical_accuracy_5: 0.8460 - dense_4_categorical_accuracy_6: 0.0713 - dense_4_categorical_accuracy_7: 0.0240 - dense_4_categorical_accuracy_8: 0.8040 - dense_4_categorical_accuracy_9: 0.0560 - dense_4_categorical_accuracy_10: 0.02 - ETA: 1:04 - loss: 21.5603 - dense_4_loss_1: 2.2003 - dense_4_loss_2: 2.0269 - dense_4_loss_3: 2.3359 - dense_4_loss_4: 2.7532 - dense_4_loss_5: 1.5124 - dense_4_loss_6: 1.7566 - dense_4_loss_7: 2.6695 - dense_4_loss_8: 1.5055 - dense_4_loss_9: 2.0284 - dense_4_loss_10: 2.7716 - dense_4_categorical_accuracy_1: 0.1069 - dense_4_categorical_accuracy_2: 0.0975 - dense_4_categorical_accuracy_3: 0.0356 - dense_4_categorical_accuracy_4: 0.0306 - dense_4_categorical_accuracy_5: 0.8081 - dense_4_categorical_accuracy_6: 0.0681 - dense_4_categorical_accuracy_7: 0.0225 - dense_4_categorical_accuracy_8: 0.8163 - dense_4_categorical_accuracy_9: 0.0525 - dense_4_categorical_accuracy_10: 0.02 - ETA: 1:00 - loss: 21.4863 - dense_4_loss_1: 2.1941 - dense_4_loss_2: 1.9909 - dense_4_loss_3: 2.3291 - dense_4_loss_4: 2.7752 - dense_4_loss_5: 1.4956 - dense_4_loss_6: 1.7478 - dense_4_loss_7: 2.6737 - dense_4_loss_8: 1.4959 - dense_4_loss_9: 2.0200 - dense_4_loss_10: 2.7640 - dense_4_categorical_accuracy_1: 0.1006 - dense_4_categorical_accuracy_2: 0.1276 - dense_4_categorical_accuracy_3: 0.0465 - dense_4_categorical_accuracy_4: 0.0335 - dense_4_categorical_accuracy_5: 0.8194 - dense_4_categorical_accuracy_6: 0.0641 - dense_4_categorical_accuracy_7: 0.0212 - dense_4_categorical_accuracy_8: 0.8271 - dense_4_categorical_accuracy_9: 0.0594 - dense_4_categorical_accuracy_10: 0.03 - ETA: 57s - loss: 21.4411 - dense_4_loss_1: 2.1843 - dense_4_loss_2: 1.9578 - dense_4_loss_3: 2.3164 - dense_4_loss_4: 2.7951 - dense_4_loss_5: 1.4758 - dense_4_loss_6: 1.7462 - dense_4_loss_7: 2.6773 - dense_4_loss_8: 1.4931 - dense_4_loss_9: 2.0134 - dense_4_loss_10: 2.7817 - dense_4_categorical_accuracy_1: 0.0950 - dense_4_categorical_accuracy_2: 0.1556 - dense_4_categorical_accuracy_3: 0.0589 - dense_4_categorical_accuracy_4: 0.0333 - dense_4_categorical_accuracy_5: 0.8294 - dense_4_categorical_accuracy_6: 0.0606 - dense_4_categorical_accuracy_7: 0.0200 - dense_4_categorical_accuracy_8: 0.8011 - dense_4_categorical_accuracy_9: 0.0706 - dense_4_categorical_accuracy_10: 0.0361 - ETA: 54s - loss: 21.3909 - dense_4_loss_1: 2.1743 - dense_4_loss_2: 1.9262 - dense_4_loss_3: 2.3065 - dense_4_loss_4: 2.8267 - dense_4_loss_5: 1.4617 - dense_4_loss_6: 1.7401 - dense_4_loss_7: 2.6870 - dense_4_loss_8: 1.4811 - dense_4_loss_9: 2.0038 - dense_4_loss_10: 2.7836 - dense_4_categorical_accuracy_1: 0.0900 - dense_4_categorical_accuracy_2: 0.1805 - dense_4_categorical_accuracy_3: 0.0637 - dense_4_categorical_accuracy_4: 0.0347 - dense_4_categorical_accuracy_5: 0.8384 - dense_4_categorical_accuracy_6: 0.0574 - dense_4_categorical_accuracy_7: 0.0189 - dense_4_categorical_accuracy_8: 0.8116 - dense_4_categorical_accuracy_9: 0.0679 - dense_4_categorical_accuracy_10: 0.038 - ETA: 52s - loss: 21.3149 - dense_4_loss_1: 2.1570 - dense_4_loss_2: 1.8940 - dense_4_loss_3: 2.2902 - dense_4_loss_4: 2.8427 - dense_4_loss_5: 1.4519 - dense_4_loss_6: 1.7331 - dense_4_loss_7: 2.6932 - dense_4_loss_8: 1.4696 - dense_4_loss_9: 1.9999 - dense_4_loss_10: 2.7832 - dense_4_categorical_accuracy_1: 0.0855 - dense_4_categorical_accuracy_2: 0.2075 - dense_4_categorical_accuracy_3: 0.0730 - dense_4_categorical_accuracy_4: 0.0380 - dense_4_categorical_accuracy_5: 0.8465 - dense_4_categorical_accuracy_6: 0.0545 - dense_4_categorical_accuracy_7: 0.0180 - dense_4_categorical_accuracy_8: 0.8210 - dense_4_categorical_accuracy_9: 0.0645 - dense_4_categorical_accuracy_10: 0.039 - ETA: 50s - loss: 21.2403 - dense_4_loss_1: 2.1470 - dense_4_loss_2: 1.8694 - dense_4_loss_3: 2.2713 - dense_4_loss_4: 2.8564 - dense_4_loss_5: 1.4412 - dense_4_loss_6: 1.7265 - dense_4_loss_7: 2.6942 - dense_4_loss_8: 1.4643 - dense_4_loss_9: 1.9936 - dense_4_loss_10: 2.7763 - dense_4_categorical_accuracy_1: 0.0814 - dense_4_categorical_accuracy_2: 0.2262 - dense_4_categorical_accuracy_3: 0.0867 - dense_4_categorical_accuracy_4: 0.0414 - dense_4_categorical_accuracy_5: 0.8533 - dense_4_categorical_accuracy_6: 0.0519 - dense_4_categorical_accuracy_7: 0.0171 - dense_4_categorical_accuracy_8: 0.8295 - dense_4_categorical_accuracy_9: 0.0614 - dense_4_categorical_accuracy_10: 0.044 - ETA: 47s - loss: 21.1744 - dense_4_loss_1: 2.1315 - dense_4_loss_2: 1.8455 - dense_4_loss_3: 2.2568 - dense_4_loss_4: 2.8713 - dense_4_loss_5: 1.4289 - dense_4_loss_6: 1.7203 - dense_4_loss_7: 2.6960 - dense_4_loss_8: 1.4651 - dense_4_loss_9: 1.9883 - dense_4_loss_10: 2.7706 - dense_4_categorical_accuracy_1: 0.0836 - dense_4_categorical_accuracy_2: 0.2459 - dense_4_categorical_accuracy_3: 0.0945 - dense_4_categorical_accuracy_4: 0.0441 - dense_4_categorical_accuracy_5: 0.8595 - dense_4_categorical_accuracy_6: 0.0495 - dense_4_categorical_accuracy_7: 0.0164 - dense_4_categorical_accuracy_8: 0.8373 - dense_4_categorical_accuracy_9: 0.0618 - dense_4_categorical_accuracy_10: 0.047 - ETA: 45s - loss: 21.1098 - dense_4_loss_1: 2.1163 - dense_4_loss_2: 1.8235 - dense_4_loss_3: 2.2479 - dense_4_loss_4: 2.8768 - dense_4_loss_5: 1.4147 - dense_4_loss_6: 1.7139 - dense_4_loss_7: 2.6998 - dense_4_loss_8: 1.4661 - dense_4_loss_9: 1.9825 - dense_4_loss_10: 2.7684 - dense_4_categorical_accuracy_1: 0.0965 - dense_4_categorical_accuracy_2: 0.2630 - dense_4_categorical_accuracy_3: 0.1000 - dense_4_categorical_accuracy_4: 0.0474 - dense_4_categorical_accuracy_5: 0.8657 - dense_4_categorical_accuracy_6: 0.0474 - dense_4_categorical_accuracy_7: 0.0157 - dense_4_categorical_accuracy_8: 0.8443 - dense_4_categorical_accuracy_9: 0.0643 - dense_4_categorical_accuracy_10: 0.051 - ETA: 44s - loss: 21.0411 - dense_4_loss_1: 2.1006 - dense_4_loss_2: 1.8019 - dense_4_loss_3: 2.2379 - dense_4_loss_4: 2.8831 - dense_4_loss_5: 1.3975 - dense_4_loss_6: 1.7090 - dense_4_loss_7: 2.7016 - dense_4_loss_8: 1.4611 - dense_4_loss_9: 1.9794 - dense_4_loss_10: 2.7688 - dense_4_categorical_accuracy_1: 0.1183 - dense_4_categorical_accuracy_2: 0.2779 - dense_4_categorical_accuracy_3: 0.1046 - dense_4_categorical_accuracy_4: 0.0479 - dense_4_categorical_accuracy_5: 0.8713 - dense_4_categorical_accuracy_6: 0.0454 - dense_4_categorical_accuracy_7: 0.0150 - dense_4_categorical_accuracy_8: 0.8508 - dense_4_categorical_accuracy_9: 0.0642 - dense_4_categorical_accuracy_10: 0.0529"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3600/10000 [=========>....................] - ETA: 42s - loss: 20.9749 - dense_4_loss_1: 2.0821 - dense_4_loss_2: 1.7799 - dense_4_loss_3: 2.2242 - dense_4_loss_4: 2.8946 - dense_4_loss_5: 1.3788 - dense_4_loss_6: 1.7052 - dense_4_loss_7: 2.7110 - dense_4_loss_8: 1.4514 - dense_4_loss_9: 1.9768 - dense_4_loss_10: 2.7709 - dense_4_categorical_accuracy_1: 0.1388 - dense_4_categorical_accuracy_2: 0.2920 - dense_4_categorical_accuracy_3: 0.1168 - dense_4_categorical_accuracy_4: 0.0460 - dense_4_categorical_accuracy_5: 0.8764 - dense_4_categorical_accuracy_6: 0.0436 - dense_4_categorical_accuracy_7: 0.0144 - dense_4_categorical_accuracy_8: 0.8568 - dense_4_categorical_accuracy_9: 0.0628 - dense_4_categorical_accuracy_10: 0.056 - ETA: 40s - loss: 20.9197 - dense_4_loss_1: 2.0674 - dense_4_loss_2: 1.7634 - dense_4_loss_3: 2.2118 - dense_4_loss_4: 2.9030 - dense_4_loss_5: 1.3629 - dense_4_loss_6: 1.7038 - dense_4_loss_7: 2.7154 - dense_4_loss_8: 1.4434 - dense_4_loss_9: 1.9711 - dense_4_loss_10: 2.7774 - dense_4_categorical_accuracy_1: 0.1546 - dense_4_categorical_accuracy_2: 0.3019 - dense_4_categorical_accuracy_3: 0.1188 - dense_4_categorical_accuracy_4: 0.0450 - dense_4_categorical_accuracy_5: 0.8812 - dense_4_categorical_accuracy_6: 0.0419 - dense_4_categorical_accuracy_7: 0.0138 - dense_4_categorical_accuracy_8: 0.8623 - dense_4_categorical_accuracy_9: 0.0685 - dense_4_categorical_accuracy_10: 0.056 - ETA: 39s - loss: 20.8345 - dense_4_loss_1: 2.0470 - dense_4_loss_2: 1.7437 - dense_4_loss_3: 2.1987 - dense_4_loss_4: 2.8992 - dense_4_loss_5: 1.3518 - dense_4_loss_6: 1.7019 - dense_4_loss_7: 2.7167 - dense_4_loss_8: 1.4424 - dense_4_loss_9: 1.9633 - dense_4_loss_10: 2.7697 - dense_4_categorical_accuracy_1: 0.1715 - dense_4_categorical_accuracy_2: 0.3133 - dense_4_categorical_accuracy_3: 0.1237 - dense_4_categorical_accuracy_4: 0.0478 - dense_4_categorical_accuracy_5: 0.8856 - dense_4_categorical_accuracy_6: 0.0404 - dense_4_categorical_accuracy_7: 0.0133 - dense_4_categorical_accuracy_8: 0.8644 - dense_4_categorical_accuracy_9: 0.0811 - dense_4_categorical_accuracy_10: 0.062 - ETA: 37s - loss: 20.7496 - dense_4_loss_1: 2.0225 - dense_4_loss_2: 1.7212 - dense_4_loss_3: 2.1843 - dense_4_loss_4: 2.8971 - dense_4_loss_5: 1.3422 - dense_4_loss_6: 1.6980 - dense_4_loss_7: 2.7154 - dense_4_loss_8: 1.4461 - dense_4_loss_9: 1.9541 - dense_4_loss_10: 2.7689 - dense_4_categorical_accuracy_1: 0.1889 - dense_4_categorical_accuracy_2: 0.3257 - dense_4_categorical_accuracy_3: 0.1364 - dense_4_categorical_accuracy_4: 0.0489 - dense_4_categorical_accuracy_5: 0.8896 - dense_4_categorical_accuracy_6: 0.0389 - dense_4_categorical_accuracy_7: 0.0129 - dense_4_categorical_accuracy_8: 0.8554 - dense_4_categorical_accuracy_9: 0.0946 - dense_4_categorical_accuracy_10: 0.062 - ETA: 36s - loss: 20.6627 - dense_4_loss_1: 1.9970 - dense_4_loss_2: 1.6984 - dense_4_loss_3: 2.1720 - dense_4_loss_4: 2.8930 - dense_4_loss_5: 1.3297 - dense_4_loss_6: 1.6972 - dense_4_loss_7: 2.7193 - dense_4_loss_8: 1.4414 - dense_4_loss_9: 1.9471 - dense_4_loss_10: 2.7675 - dense_4_categorical_accuracy_1: 0.2048 - dense_4_categorical_accuracy_2: 0.3369 - dense_4_categorical_accuracy_3: 0.1459 - dense_4_categorical_accuracy_4: 0.0524 - dense_4_categorical_accuracy_5: 0.8934 - dense_4_categorical_accuracy_6: 0.0376 - dense_4_categorical_accuracy_7: 0.0124 - dense_4_categorical_accuracy_8: 0.8579 - dense_4_categorical_accuracy_9: 0.0990 - dense_4_categorical_accuracy_10: 0.067 - ETA: 35s - loss: 20.5772 - dense_4_loss_1: 1.9729 - dense_4_loss_2: 1.6776 - dense_4_loss_3: 2.1569 - dense_4_loss_4: 2.8885 - dense_4_loss_5: 1.3151 - dense_4_loss_6: 1.6968 - dense_4_loss_7: 2.7312 - dense_4_loss_8: 1.4277 - dense_4_loss_9: 1.9446 - dense_4_loss_10: 2.7658 - dense_4_categorical_accuracy_1: 0.2183 - dense_4_categorical_accuracy_2: 0.3460 - dense_4_categorical_accuracy_3: 0.1550 - dense_4_categorical_accuracy_4: 0.0560 - dense_4_categorical_accuracy_5: 0.8970 - dense_4_categorical_accuracy_6: 0.0363 - dense_4_categorical_accuracy_7: 0.0120 - dense_4_categorical_accuracy_8: 0.8627 - dense_4_categorical_accuracy_9: 0.0963 - dense_4_categorical_accuracy_10: 0.069 - ETA: 34s - loss: 20.4916 - dense_4_loss_1: 1.9453 - dense_4_loss_2: 1.6552 - dense_4_loss_3: 2.1455 - dense_4_loss_4: 2.8895 - dense_4_loss_5: 1.3014 - dense_4_loss_6: 1.6936 - dense_4_loss_7: 2.7373 - dense_4_loss_8: 1.4270 - dense_4_loss_9: 1.9375 - dense_4_loss_10: 2.7593 - dense_4_categorical_accuracy_1: 0.2319 - dense_4_categorical_accuracy_2: 0.3561 - dense_4_categorical_accuracy_3: 0.1590 - dense_4_categorical_accuracy_4: 0.0548 - dense_4_categorical_accuracy_5: 0.8997 - dense_4_categorical_accuracy_6: 0.0361 - dense_4_categorical_accuracy_7: 0.0116 - dense_4_categorical_accuracy_8: 0.8548 - dense_4_categorical_accuracy_9: 0.1045 - dense_4_categorical_accuracy_10: 0.072 - ETA: 33s - loss: 20.4244 - dense_4_loss_1: 1.9210 - dense_4_loss_2: 1.6348 - dense_4_loss_3: 2.1293 - dense_4_loss_4: 2.8890 - dense_4_loss_5: 1.2903 - dense_4_loss_6: 1.6925 - dense_4_loss_7: 2.7410 - dense_4_loss_8: 1.4361 - dense_4_loss_9: 1.9273 - dense_4_loss_10: 2.7630 - dense_4_categorical_accuracy_1: 0.2416 - dense_4_categorical_accuracy_2: 0.3644 - dense_4_categorical_accuracy_3: 0.1684 - dense_4_categorical_accuracy_4: 0.0541 - dense_4_categorical_accuracy_5: 0.9025 - dense_4_categorical_accuracy_6: 0.0362 - dense_4_categorical_accuracy_7: 0.0128 - dense_4_categorical_accuracy_8: 0.8291 - dense_4_categorical_accuracy_9: 0.1137 - dense_4_categorical_accuracy_10: 0.073 - ETA: 32s - loss: 20.3322 - dense_4_loss_1: 1.8913 - dense_4_loss_2: 1.6106 - dense_4_loss_3: 2.1148 - dense_4_loss_4: 2.8881 - dense_4_loss_5: 1.2752 - dense_4_loss_6: 1.6899 - dense_4_loss_7: 2.7504 - dense_4_loss_8: 1.4279 - dense_4_loss_9: 1.9246 - dense_4_loss_10: 2.7593 - dense_4_categorical_accuracy_1: 0.2542 - dense_4_categorical_accuracy_2: 0.3767 - dense_4_categorical_accuracy_3: 0.1752 - dense_4_categorical_accuracy_4: 0.0533 - dense_4_categorical_accuracy_5: 0.9055 - dense_4_categorical_accuracy_6: 0.0352 - dense_4_categorical_accuracy_7: 0.0124 - dense_4_categorical_accuracy_8: 0.8309 - dense_4_categorical_accuracy_9: 0.1139 - dense_4_categorical_accuracy_10: 0.074 - ETA: 31s - loss: 20.2476 - dense_4_loss_1: 1.8633 - dense_4_loss_2: 1.5857 - dense_4_loss_3: 2.1054 - dense_4_loss_4: 2.8795 - dense_4_loss_5: 1.2650 - dense_4_loss_6: 1.6894 - dense_4_loss_7: 2.7605 - dense_4_loss_8: 1.4154 - dense_4_loss_9: 1.9257 - dense_4_loss_10: 2.7578 - dense_4_categorical_accuracy_1: 0.2647 - dense_4_categorical_accuracy_2: 0.3868 - dense_4_categorical_accuracy_3: 0.1797 - dense_4_categorical_accuracy_4: 0.0547 - dense_4_categorical_accuracy_5: 0.9071 - dense_4_categorical_accuracy_6: 0.0344 - dense_4_categorical_accuracy_7: 0.0121 - dense_4_categorical_accuracy_8: 0.8344 - dense_4_categorical_accuracy_9: 0.1112 - dense_4_categorical_accuracy_10: 0.074 - ETA: 30s - loss: 20.1636 - dense_4_loss_1: 1.8342 - dense_4_loss_2: 1.5623 - dense_4_loss_3: 2.0958 - dense_4_loss_4: 2.8795 - dense_4_loss_5: 1.2527 - dense_4_loss_6: 1.6881 - dense_4_loss_7: 2.7588 - dense_4_loss_8: 1.4185 - dense_4_loss_9: 1.9196 - dense_4_loss_10: 2.7542 - dense_4_categorical_accuracy_1: 0.2760 - dense_4_categorical_accuracy_2: 0.4009 - dense_4_categorical_accuracy_3: 0.1829 - dense_4_categorical_accuracy_4: 0.0543 - dense_4_categorical_accuracy_5: 0.9083 - dense_4_categorical_accuracy_6: 0.0343 - dense_4_categorical_accuracy_7: 0.0123 - dense_4_categorical_accuracy_8: 0.8297 - dense_4_categorical_accuracy_9: 0.1177 - dense_4_categorical_accuracy_10: 0.076 - ETA: 29s - loss: 20.0804 - dense_4_loss_1: 1.8071 - dense_4_loss_2: 1.5387 - dense_4_loss_3: 2.0833 - dense_4_loss_4: 2.8769 - dense_4_loss_5: 1.2424 - dense_4_loss_6: 1.6839 - dense_4_loss_7: 2.7596 - dense_4_loss_8: 1.4205 - dense_4_loss_9: 1.9155 - dense_4_loss_10: 2.7524 - dense_4_categorical_accuracy_1: 0.2847 - dense_4_categorical_accuracy_2: 0.4117 - dense_4_categorical_accuracy_3: 0.1892 - dense_4_categorical_accuracy_4: 0.0531 - dense_4_categorical_accuracy_5: 0.9108 - dense_4_categorical_accuracy_6: 0.0347 - dense_4_categorical_accuracy_7: 0.0122 - dense_4_categorical_accuracy_8: 0.8267 - dense_4_categorical_accuracy_9: 0.1233 - dense_4_categorical_accuracy_10: 0.0772"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4800/10000 [=============>................] - ETA: 28s - loss: 20.0005 - dense_4_loss_1: 1.7798 - dense_4_loss_2: 1.5160 - dense_4_loss_3: 2.0715 - dense_4_loss_4: 2.8727 - dense_4_loss_5: 1.2333 - dense_4_loss_6: 1.6782 - dense_4_loss_7: 2.7643 - dense_4_loss_8: 1.4187 - dense_4_loss_9: 1.9131 - dense_4_loss_10: 2.7529 - dense_4_categorical_accuracy_1: 0.2941 - dense_4_categorical_accuracy_2: 0.4203 - dense_4_categorical_accuracy_3: 0.1927 - dense_4_categorical_accuracy_4: 0.0538 - dense_4_categorical_accuracy_5: 0.9132 - dense_4_categorical_accuracy_6: 0.0368 - dense_4_categorical_accuracy_7: 0.0127 - dense_4_categorical_accuracy_8: 0.8219 - dense_4_categorical_accuracy_9: 0.1278 - dense_4_categorical_accuracy_10: 0.077 - ETA: 27s - loss: 19.9094 - dense_4_loss_1: 1.7526 - dense_4_loss_2: 1.4924 - dense_4_loss_3: 2.0607 - dense_4_loss_4: 2.8700 - dense_4_loss_5: 1.2183 - dense_4_loss_6: 1.6710 - dense_4_loss_7: 2.7706 - dense_4_loss_8: 1.4087 - dense_4_loss_9: 1.9140 - dense_4_loss_10: 2.7510 - dense_4_categorical_accuracy_1: 0.3037 - dense_4_categorical_accuracy_2: 0.4313 - dense_4_categorical_accuracy_3: 0.1961 - dense_4_categorical_accuracy_4: 0.0524 - dense_4_categorical_accuracy_5: 0.9155 - dense_4_categorical_accuracy_6: 0.0358 - dense_4_categorical_accuracy_7: 0.0124 - dense_4_categorical_accuracy_8: 0.8266 - dense_4_categorical_accuracy_9: 0.1250 - dense_4_categorical_accuracy_10: 0.077 - ETA: 26s - loss: 19.8194 - dense_4_loss_1: 1.7273 - dense_4_loss_2: 1.4709 - dense_4_loss_3: 2.0472 - dense_4_loss_4: 2.8684 - dense_4_loss_5: 1.2030 - dense_4_loss_6: 1.6641 - dense_4_loss_7: 2.7765 - dense_4_loss_8: 1.3987 - dense_4_loss_9: 1.9168 - dense_4_loss_10: 2.7466 - dense_4_categorical_accuracy_1: 0.3118 - dense_4_categorical_accuracy_2: 0.4413 - dense_4_categorical_accuracy_3: 0.2021 - dense_4_categorical_accuracy_4: 0.0510 - dense_4_categorical_accuracy_5: 0.9177 - dense_4_categorical_accuracy_6: 0.0369 - dense_4_categorical_accuracy_7: 0.0123 - dense_4_categorical_accuracy_8: 0.8310 - dense_4_categorical_accuracy_9: 0.1218 - dense_4_categorical_accuracy_10: 0.076 - ETA: 25s - loss: 19.7199 - dense_4_loss_1: 1.7020 - dense_4_loss_2: 1.4492 - dense_4_loss_3: 2.0326 - dense_4_loss_4: 2.8632 - dense_4_loss_5: 1.1884 - dense_4_loss_6: 1.6550 - dense_4_loss_7: 2.7755 - dense_4_loss_8: 1.3962 - dense_4_loss_9: 1.9174 - dense_4_loss_10: 2.7405 - dense_4_categorical_accuracy_1: 0.3203 - dense_4_categorical_accuracy_2: 0.4525 - dense_4_categorical_accuracy_3: 0.2077 - dense_4_categorical_accuracy_4: 0.0512 - dense_4_categorical_accuracy_5: 0.9198 - dense_4_categorical_accuracy_6: 0.0480 - dense_4_categorical_accuracy_7: 0.0138 - dense_4_categorical_accuracy_8: 0.8353 - dense_4_categorical_accuracy_9: 0.1222 - dense_4_categorical_accuracy_10: 0.078 - ETA: 25s - loss: 19.6167 - dense_4_loss_1: 1.6774 - dense_4_loss_2: 1.4271 - dense_4_loss_3: 2.0173 - dense_4_loss_4: 2.8561 - dense_4_loss_5: 1.1746 - dense_4_loss_6: 1.6434 - dense_4_loss_7: 2.7731 - dense_4_loss_8: 1.3972 - dense_4_loss_9: 1.9169 - dense_4_loss_10: 2.7337 - dense_4_categorical_accuracy_1: 0.3283 - dense_4_categorical_accuracy_2: 0.4629 - dense_4_categorical_accuracy_3: 0.2141 - dense_4_categorical_accuracy_4: 0.0524 - dense_4_categorical_accuracy_5: 0.9215 - dense_4_categorical_accuracy_6: 0.0632 - dense_4_categorical_accuracy_7: 0.0151 - dense_4_categorical_accuracy_8: 0.8356 - dense_4_categorical_accuracy_9: 0.1273 - dense_4_categorical_accuracy_10: 0.079 - ETA: 24s - loss: 19.5095 - dense_4_loss_1: 1.6527 - dense_4_loss_2: 1.4056 - dense_4_loss_3: 2.0036 - dense_4_loss_4: 2.8521 - dense_4_loss_5: 1.1568 - dense_4_loss_6: 1.6318 - dense_4_loss_7: 2.7758 - dense_4_loss_8: 1.3877 - dense_4_loss_9: 1.9153 - dense_4_loss_10: 2.7282 - dense_4_categorical_accuracy_1: 0.3369 - dense_4_categorical_accuracy_2: 0.4726 - dense_4_categorical_accuracy_3: 0.2181 - dense_4_categorical_accuracy_4: 0.0531 - dense_4_categorical_accuracy_5: 0.9233 - dense_4_categorical_accuracy_6: 0.0786 - dense_4_categorical_accuracy_7: 0.0150 - dense_4_categorical_accuracy_8: 0.8395 - dense_4_categorical_accuracy_9: 0.1345 - dense_4_categorical_accuracy_10: 0.080 - ETA: 23s - loss: 19.3960 - dense_4_loss_1: 1.6279 - dense_4_loss_2: 1.3833 - dense_4_loss_3: 1.9892 - dense_4_loss_4: 2.8485 - dense_4_loss_5: 1.1388 - dense_4_loss_6: 1.6174 - dense_4_loss_7: 2.7846 - dense_4_loss_8: 1.3681 - dense_4_loss_9: 1.9118 - dense_4_loss_10: 2.7263 - dense_4_categorical_accuracy_1: 0.3470 - dense_4_categorical_accuracy_2: 0.4821 - dense_4_categorical_accuracy_3: 0.2226 - dense_4_categorical_accuracy_4: 0.0526 - dense_4_categorical_accuracy_5: 0.9251 - dense_4_categorical_accuracy_6: 0.0942 - dense_4_categorical_accuracy_7: 0.0147 - dense_4_categorical_accuracy_8: 0.8433 - dense_4_categorical_accuracy_9: 0.1374 - dense_4_categorical_accuracy_10: 0.082 - ETA: 22s - loss: 19.2734 - dense_4_loss_1: 1.6043 - dense_4_loss_2: 1.3619 - dense_4_loss_3: 1.9772 - dense_4_loss_4: 2.8429 - dense_4_loss_5: 1.1231 - dense_4_loss_6: 1.6004 - dense_4_loss_7: 2.7870 - dense_4_loss_8: 1.3482 - dense_4_loss_9: 1.9034 - dense_4_loss_10: 2.7251 - dense_4_categorical_accuracy_1: 0.3561 - dense_4_categorical_accuracy_2: 0.4909 - dense_4_categorical_accuracy_3: 0.2261 - dense_4_categorical_accuracy_4: 0.0516 - dense_4_categorical_accuracy_5: 0.9268 - dense_4_categorical_accuracy_6: 0.1080 - dense_4_categorical_accuracy_7: 0.0148 - dense_4_categorical_accuracy_8: 0.8468 - dense_4_categorical_accuracy_9: 0.1445 - dense_4_categorical_accuracy_10: 0.083 - ETA: 22s - loss: 19.1435 - dense_4_loss_1: 1.5804 - dense_4_loss_2: 1.3392 - dense_4_loss_3: 1.9649 - dense_4_loss_4: 2.8375 - dense_4_loss_5: 1.1100 - dense_4_loss_6: 1.5817 - dense_4_loss_7: 2.7884 - dense_4_loss_8: 1.3305 - dense_4_loss_9: 1.8927 - dense_4_loss_10: 2.7181 - dense_4_categorical_accuracy_1: 0.3671 - dense_4_categorical_accuracy_2: 0.5013 - dense_4_categorical_accuracy_3: 0.2296 - dense_4_categorical_accuracy_4: 0.0527 - dense_4_categorical_accuracy_5: 0.9282 - dense_4_categorical_accuracy_6: 0.1213 - dense_4_categorical_accuracy_7: 0.0158 - dense_4_categorical_accuracy_8: 0.8502 - dense_4_categorical_accuracy_9: 0.1518 - dense_4_categorical_accuracy_10: 0.084 - ETA: 21s - loss: 19.0127 - dense_4_loss_1: 1.5586 - dense_4_loss_2: 1.3190 - dense_4_loss_3: 1.9526 - dense_4_loss_4: 2.8314 - dense_4_loss_5: 1.0945 - dense_4_loss_6: 1.5621 - dense_4_loss_7: 2.7883 - dense_4_loss_8: 1.3127 - dense_4_loss_9: 1.8826 - dense_4_loss_10: 2.7108 - dense_4_categorical_accuracy_1: 0.3761 - dense_4_categorical_accuracy_2: 0.5100 - dense_4_categorical_accuracy_3: 0.2337 - dense_4_categorical_accuracy_4: 0.0524 - dense_4_categorical_accuracy_5: 0.9298 - dense_4_categorical_accuracy_6: 0.1350 - dense_4_categorical_accuracy_7: 0.0157 - dense_4_categorical_accuracy_8: 0.8535 - dense_4_categorical_accuracy_9: 0.1578 - dense_4_categorical_accuracy_10: 0.085 - ETA: 21s - loss: 18.8795 - dense_4_loss_1: 1.5356 - dense_4_loss_2: 1.2970 - dense_4_loss_3: 1.9415 - dense_4_loss_4: 2.8255 - dense_4_loss_5: 1.0767 - dense_4_loss_6: 1.5446 - dense_4_loss_7: 2.7869 - dense_4_loss_8: 1.2917 - dense_4_loss_9: 1.8744 - dense_4_loss_10: 2.7055 - dense_4_categorical_accuracy_1: 0.3870 - dense_4_categorical_accuracy_2: 0.5196 - dense_4_categorical_accuracy_3: 0.2377 - dense_4_categorical_accuracy_4: 0.0553 - dense_4_categorical_accuracy_5: 0.9313 - dense_4_categorical_accuracy_6: 0.1474 - dense_4_categorical_accuracy_7: 0.0172 - dense_4_categorical_accuracy_8: 0.8566 - dense_4_categorical_accuracy_9: 0.1621 - dense_4_categorical_accuracy_10: 0.086 - ETA: 20s - loss: 18.7576 - dense_4_loss_1: 1.5140 - dense_4_loss_2: 1.2770 - dense_4_loss_3: 1.9302 - dense_4_loss_4: 2.8240 - dense_4_loss_5: 1.0579 - dense_4_loss_6: 1.5281 - dense_4_loss_7: 2.7931 - dense_4_loss_8: 1.2680 - dense_4_loss_9: 1.8649 - dense_4_loss_10: 2.7002 - dense_4_categorical_accuracy_1: 0.3973 - dense_4_categorical_accuracy_2: 0.5285 - dense_4_categorical_accuracy_3: 0.2398 - dense_4_categorical_accuracy_4: 0.0542 - dense_4_categorical_accuracy_5: 0.9327 - dense_4_categorical_accuracy_6: 0.1585 - dense_4_categorical_accuracy_7: 0.0169 - dense_4_categorical_accuracy_8: 0.8596 - dense_4_categorical_accuracy_9: 0.1660 - dense_4_categorical_accuracy_10: 0.0869"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6000/10000 [=================>............] - ETA: 19s - loss: 18.6282 - dense_4_loss_1: 1.4939 - dense_4_loss_2: 1.2608 - dense_4_loss_3: 1.9193 - dense_4_loss_4: 2.8197 - dense_4_loss_5: 1.0385 - dense_4_loss_6: 1.5096 - dense_4_loss_7: 2.7951 - dense_4_loss_8: 1.2448 - dense_4_loss_9: 1.8538 - dense_4_loss_10: 2.6926 - dense_4_categorical_accuracy_1: 0.4078 - dense_4_categorical_accuracy_2: 0.5341 - dense_4_categorical_accuracy_3: 0.2422 - dense_4_categorical_accuracy_4: 0.0539 - dense_4_categorical_accuracy_5: 0.9341 - dense_4_categorical_accuracy_6: 0.1710 - dense_4_categorical_accuracy_7: 0.0167 - dense_4_categorical_accuracy_8: 0.8624 - dense_4_categorical_accuracy_9: 0.1710 - dense_4_categorical_accuracy_10: 0.087 - ETA: 19s - loss: 18.4991 - dense_4_loss_1: 1.4734 - dense_4_loss_2: 1.2426 - dense_4_loss_3: 1.9076 - dense_4_loss_4: 2.8162 - dense_4_loss_5: 1.0199 - dense_4_loss_6: 1.4945 - dense_4_loss_7: 2.7891 - dense_4_loss_8: 1.2234 - dense_4_loss_9: 1.8436 - dense_4_loss_10: 2.6889 - dense_4_categorical_accuracy_1: 0.4166 - dense_4_categorical_accuracy_2: 0.5414 - dense_4_categorical_accuracy_3: 0.2444 - dense_4_categorical_accuracy_4: 0.0546 - dense_4_categorical_accuracy_5: 0.9354 - dense_4_categorical_accuracy_6: 0.1826 - dense_4_categorical_accuracy_7: 0.0188 - dense_4_categorical_accuracy_8: 0.8652 - dense_4_categorical_accuracy_9: 0.1742 - dense_4_categorical_accuracy_10: 0.087 - ETA: 18s - loss: 18.3706 - dense_4_loss_1: 1.4558 - dense_4_loss_2: 1.2287 - dense_4_loss_3: 1.8979 - dense_4_loss_4: 2.8101 - dense_4_loss_5: 1.0011 - dense_4_loss_6: 1.4780 - dense_4_loss_7: 2.7830 - dense_4_loss_8: 1.2017 - dense_4_loss_9: 1.8329 - dense_4_loss_10: 2.6814 - dense_4_categorical_accuracy_1: 0.4224 - dense_4_categorical_accuracy_2: 0.5455 - dense_4_categorical_accuracy_3: 0.2461 - dense_4_categorical_accuracy_4: 0.0557 - dense_4_categorical_accuracy_5: 0.9367 - dense_4_categorical_accuracy_6: 0.1951 - dense_4_categorical_accuracy_7: 0.0194 - dense_4_categorical_accuracy_8: 0.8678 - dense_4_categorical_accuracy_9: 0.1775 - dense_4_categorical_accuracy_10: 0.087 - ETA: 18s - loss: 18.2319 - dense_4_loss_1: 1.4364 - dense_4_loss_2: 1.2114 - dense_4_loss_3: 1.8853 - dense_4_loss_4: 2.8021 - dense_4_loss_5: 0.9827 - dense_4_loss_6: 1.4621 - dense_4_loss_7: 2.7745 - dense_4_loss_8: 1.1804 - dense_4_loss_9: 1.8220 - dense_4_loss_10: 2.6749 - dense_4_categorical_accuracy_1: 0.4304 - dense_4_categorical_accuracy_2: 0.5535 - dense_4_categorical_accuracy_3: 0.2506 - dense_4_categorical_accuracy_4: 0.0569 - dense_4_categorical_accuracy_5: 0.9379 - dense_4_categorical_accuracy_6: 0.2054 - dense_4_categorical_accuracy_7: 0.0210 - dense_4_categorical_accuracy_8: 0.8704 - dense_4_categorical_accuracy_9: 0.1819 - dense_4_categorical_accuracy_10: 0.087 - ETA: 17s - loss: 18.1020 - dense_4_loss_1: 1.4198 - dense_4_loss_2: 1.1962 - dense_4_loss_3: 1.8733 - dense_4_loss_4: 2.7945 - dense_4_loss_5: 0.9648 - dense_4_loss_6: 1.4461 - dense_4_loss_7: 2.7669 - dense_4_loss_8: 1.1595 - dense_4_loss_9: 1.8117 - dense_4_loss_10: 2.6691 - dense_4_categorical_accuracy_1: 0.4366 - dense_4_categorical_accuracy_2: 0.5606 - dense_4_categorical_accuracy_3: 0.2547 - dense_4_categorical_accuracy_4: 0.0579 - dense_4_categorical_accuracy_5: 0.9391 - dense_4_categorical_accuracy_6: 0.2158 - dense_4_categorical_accuracy_7: 0.0236 - dense_4_categorical_accuracy_8: 0.8728 - dense_4_categorical_accuracy_9: 0.1845 - dense_4_categorical_accuracy_10: 0.088 - ETA: 17s - loss: 17.9845 - dense_4_loss_1: 1.4029 - dense_4_loss_2: 1.1841 - dense_4_loss_3: 1.8653 - dense_4_loss_4: 2.7868 - dense_4_loss_5: 0.9475 - dense_4_loss_6: 1.4298 - dense_4_loss_7: 2.7596 - dense_4_loss_8: 1.1393 - dense_4_loss_9: 1.8051 - dense_4_loss_10: 2.6642 - dense_4_categorical_accuracy_1: 0.4450 - dense_4_categorical_accuracy_2: 0.5654 - dense_4_categorical_accuracy_3: 0.2559 - dense_4_categorical_accuracy_4: 0.0596 - dense_4_categorical_accuracy_5: 0.9402 - dense_4_categorical_accuracy_6: 0.2265 - dense_4_categorical_accuracy_7: 0.0254 - dense_4_categorical_accuracy_8: 0.8752 - dense_4_categorical_accuracy_9: 0.1859 - dense_4_categorical_accuracy_10: 0.087 - ETA: 16s - loss: 17.8604 - dense_4_loss_1: 1.3870 - dense_4_loss_2: 1.1699 - dense_4_loss_3: 1.8550 - dense_4_loss_4: 2.7781 - dense_4_loss_5: 0.9308 - dense_4_loss_6: 1.4140 - dense_4_loss_7: 2.7514 - dense_4_loss_8: 1.1198 - dense_4_loss_9: 1.7968 - dense_4_loss_10: 2.6576 - dense_4_categorical_accuracy_1: 0.4513 - dense_4_categorical_accuracy_2: 0.5720 - dense_4_categorical_accuracy_3: 0.2589 - dense_4_categorical_accuracy_4: 0.0609 - dense_4_categorical_accuracy_5: 0.9413 - dense_4_categorical_accuracy_6: 0.2365 - dense_4_categorical_accuracy_7: 0.0275 - dense_4_categorical_accuracy_8: 0.8775 - dense_4_categorical_accuracy_9: 0.1889 - dense_4_categorical_accuracy_10: 0.087 - ETA: 16s - loss: 17.7344 - dense_4_loss_1: 1.3699 - dense_4_loss_2: 1.1544 - dense_4_loss_3: 1.8443 - dense_4_loss_4: 2.7705 - dense_4_loss_5: 0.9146 - dense_4_loss_6: 1.3999 - dense_4_loss_7: 2.7414 - dense_4_loss_8: 1.1007 - dense_4_loss_9: 1.7882 - dense_4_loss_10: 2.6506 - dense_4_categorical_accuracy_1: 0.4589 - dense_4_categorical_accuracy_2: 0.5789 - dense_4_categorical_accuracy_3: 0.2612 - dense_4_categorical_accuracy_4: 0.0627 - dense_4_categorical_accuracy_5: 0.9423 - dense_4_categorical_accuracy_6: 0.2452 - dense_4_categorical_accuracy_7: 0.0307 - dense_4_categorical_accuracy_8: 0.8796 - dense_4_categorical_accuracy_9: 0.1934 - dense_4_categorical_accuracy_10: 0.088 - ETA: 15s - loss: 17.6156 - dense_4_loss_1: 1.3538 - dense_4_loss_2: 1.1400 - dense_4_loss_3: 1.8345 - dense_4_loss_4: 2.7628 - dense_4_loss_5: 0.8990 - dense_4_loss_6: 1.3843 - dense_4_loss_7: 2.7332 - dense_4_loss_8: 1.0821 - dense_4_loss_9: 1.7803 - dense_4_loss_10: 2.6456 - dense_4_categorical_accuracy_1: 0.4649 - dense_4_categorical_accuracy_2: 0.5846 - dense_4_categorical_accuracy_3: 0.2637 - dense_4_categorical_accuracy_4: 0.0644 - dense_4_categorical_accuracy_5: 0.9433 - dense_4_categorical_accuracy_6: 0.2549 - dense_4_categorical_accuracy_7: 0.0332 - dense_4_categorical_accuracy_8: 0.8818 - dense_4_categorical_accuracy_9: 0.1956 - dense_4_categorical_accuracy_10: 0.088 - ETA: 15s - loss: 17.5016 - dense_4_loss_1: 1.3381 - dense_4_loss_2: 1.1259 - dense_4_loss_3: 1.8258 - dense_4_loss_4: 2.7551 - dense_4_loss_5: 0.8840 - dense_4_loss_6: 1.3703 - dense_4_loss_7: 2.7247 - dense_4_loss_8: 1.0641 - dense_4_loss_9: 1.7740 - dense_4_loss_10: 2.6395 - dense_4_categorical_accuracy_1: 0.4709 - dense_4_categorical_accuracy_2: 0.5900 - dense_4_categorical_accuracy_3: 0.2650 - dense_4_categorical_accuracy_4: 0.0652 - dense_4_categorical_accuracy_5: 0.9443 - dense_4_categorical_accuracy_6: 0.2633 - dense_4_categorical_accuracy_7: 0.0357 - dense_4_categorical_accuracy_8: 0.8838 - dense_4_categorical_accuracy_9: 0.1986 - dense_4_categorical_accuracy_10: 0.088 - ETA: 14s - loss: 17.3878 - dense_4_loss_1: 1.3241 - dense_4_loss_2: 1.1133 - dense_4_loss_3: 1.8171 - dense_4_loss_4: 2.7476 - dense_4_loss_5: 0.8695 - dense_4_loss_6: 1.3566 - dense_4_loss_7: 2.7139 - dense_4_loss_8: 1.0466 - dense_4_loss_9: 1.7657 - dense_4_loss_10: 2.6334 - dense_4_categorical_accuracy_1: 0.4751 - dense_4_categorical_accuracy_2: 0.5941 - dense_4_categorical_accuracy_3: 0.2669 - dense_4_categorical_accuracy_4: 0.0659 - dense_4_categorical_accuracy_5: 0.9453 - dense_4_categorical_accuracy_6: 0.2712 - dense_4_categorical_accuracy_7: 0.0398 - dense_4_categorical_accuracy_8: 0.8858 - dense_4_categorical_accuracy_9: 0.2017 - dense_4_categorical_accuracy_10: 0.088 - ETA: 14s - loss: 17.2710 - dense_4_loss_1: 1.3085 - dense_4_loss_2: 1.0984 - dense_4_loss_3: 1.8069 - dense_4_loss_4: 2.7380 - dense_4_loss_5: 0.8554 - dense_4_loss_6: 1.3422 - dense_4_loss_7: 2.7048 - dense_4_loss_8: 1.0297 - dense_4_loss_9: 1.7594 - dense_4_loss_10: 2.6276 - dense_4_categorical_accuracy_1: 0.4823 - dense_4_categorical_accuracy_2: 0.6002 - dense_4_categorical_accuracy_3: 0.2690 - dense_4_categorical_accuracy_4: 0.0680 - dense_4_categorical_accuracy_5: 0.9462 - dense_4_categorical_accuracy_6: 0.2795 - dense_4_categorical_accuracy_7: 0.0428 - dense_4_categorical_accuracy_8: 0.8877 - dense_4_categorical_accuracy_9: 0.2048 - dense_4_categorical_accuracy_10: 0.0902"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7200/10000 [====================>.........] - ETA: 13s - loss: 17.1575 - dense_4_loss_1: 1.2937 - dense_4_loss_2: 1.0840 - dense_4_loss_3: 1.7963 - dense_4_loss_4: 2.7308 - dense_4_loss_5: 0.8418 - dense_4_loss_6: 1.3293 - dense_4_loss_7: 2.6955 - dense_4_loss_8: 1.0133 - dense_4_loss_9: 1.7511 - dense_4_loss_10: 2.6216 - dense_4_categorical_accuracy_1: 0.4890 - dense_4_categorical_accuracy_2: 0.6061 - dense_4_categorical_accuracy_3: 0.2713 - dense_4_categorical_accuracy_4: 0.0689 - dense_4_categorical_accuracy_5: 0.9470 - dense_4_categorical_accuracy_6: 0.2870 - dense_4_categorical_accuracy_7: 0.0464 - dense_4_categorical_accuracy_8: 0.8895 - dense_4_categorical_accuracy_9: 0.2082 - dense_4_categorical_accuracy_10: 0.091 - ETA: 13s - loss: 17.0535 - dense_4_loss_1: 1.2787 - dense_4_loss_2: 1.0711 - dense_4_loss_3: 1.7876 - dense_4_loss_4: 2.7257 - dense_4_loss_5: 0.8287 - dense_4_loss_6: 1.3174 - dense_4_loss_7: 2.6867 - dense_4_loss_8: 0.9976 - dense_4_loss_9: 1.7439 - dense_4_loss_10: 2.6161 - dense_4_categorical_accuracy_1: 0.4963 - dense_4_categorical_accuracy_2: 0.6110 - dense_4_categorical_accuracy_3: 0.2737 - dense_4_categorical_accuracy_4: 0.0695 - dense_4_categorical_accuracy_5: 0.9479 - dense_4_categorical_accuracy_6: 0.2940 - dense_4_categorical_accuracy_7: 0.0489 - dense_4_categorical_accuracy_8: 0.8913 - dense_4_categorical_accuracy_9: 0.2124 - dense_4_categorical_accuracy_10: 0.093 - ETA: 12s - loss: 16.9518 - dense_4_loss_1: 1.2640 - dense_4_loss_2: 1.0571 - dense_4_loss_3: 1.7772 - dense_4_loss_4: 2.7201 - dense_4_loss_5: 0.8160 - dense_4_loss_6: 1.3052 - dense_4_loss_7: 2.6803 - dense_4_loss_8: 0.9823 - dense_4_loss_9: 1.7384 - dense_4_loss_10: 2.6113 - dense_4_categorical_accuracy_1: 0.5038 - dense_4_categorical_accuracy_2: 0.6165 - dense_4_categorical_accuracy_3: 0.2759 - dense_4_categorical_accuracy_4: 0.0710 - dense_4_categorical_accuracy_5: 0.9487 - dense_4_categorical_accuracy_6: 0.3027 - dense_4_categorical_accuracy_7: 0.0497 - dense_4_categorical_accuracy_8: 0.8930 - dense_4_categorical_accuracy_9: 0.2159 - dense_4_categorical_accuracy_10: 0.094 - ETA: 12s - loss: 16.8484 - dense_4_loss_1: 1.2494 - dense_4_loss_2: 1.0437 - dense_4_loss_3: 1.7682 - dense_4_loss_4: 2.7131 - dense_4_loss_5: 0.8036 - dense_4_loss_6: 1.2940 - dense_4_loss_7: 2.6723 - dense_4_loss_8: 0.9674 - dense_4_loss_9: 1.7302 - dense_4_loss_10: 2.6064 - dense_4_categorical_accuracy_1: 0.5106 - dense_4_categorical_accuracy_2: 0.6216 - dense_4_categorical_accuracy_3: 0.2780 - dense_4_categorical_accuracy_4: 0.0725 - dense_4_categorical_accuracy_5: 0.9495 - dense_4_categorical_accuracy_6: 0.3097 - dense_4_categorical_accuracy_7: 0.0520 - dense_4_categorical_accuracy_8: 0.8947 - dense_4_categorical_accuracy_9: 0.2200 - dense_4_categorical_accuracy_10: 0.095 - ETA: 12s - loss: 16.7446 - dense_4_loss_1: 1.2345 - dense_4_loss_2: 1.0301 - dense_4_loss_3: 1.7585 - dense_4_loss_4: 2.7070 - dense_4_loss_5: 0.7915 - dense_4_loss_6: 1.2825 - dense_4_loss_7: 2.6649 - dense_4_loss_8: 0.9529 - dense_4_loss_9: 1.7216 - dense_4_loss_10: 2.6010 - dense_4_categorical_accuracy_1: 0.5175 - dense_4_categorical_accuracy_2: 0.6269 - dense_4_categorical_accuracy_3: 0.2806 - dense_4_categorical_accuracy_4: 0.0738 - dense_4_categorical_accuracy_5: 0.9503 - dense_4_categorical_accuracy_6: 0.3174 - dense_4_categorical_accuracy_7: 0.0549 - dense_4_categorical_accuracy_8: 0.8963 - dense_4_categorical_accuracy_9: 0.2226 - dense_4_categorical_accuracy_10: 0.095 - ETA: 11s - loss: 16.6488 - dense_4_loss_1: 1.2215 - dense_4_loss_2: 1.0189 - dense_4_loss_3: 1.7501 - dense_4_loss_4: 2.7022 - dense_4_loss_5: 0.7798 - dense_4_loss_6: 1.2702 - dense_4_loss_7: 2.6566 - dense_4_loss_8: 0.9388 - dense_4_loss_9: 1.7140 - dense_4_loss_10: 2.5965 - dense_4_categorical_accuracy_1: 0.5232 - dense_4_categorical_accuracy_2: 0.6311 - dense_4_categorical_accuracy_3: 0.2829 - dense_4_categorical_accuracy_4: 0.0752 - dense_4_categorical_accuracy_5: 0.9511 - dense_4_categorical_accuracy_6: 0.3262 - dense_4_categorical_accuracy_7: 0.0573 - dense_4_categorical_accuracy_8: 0.8979 - dense_4_categorical_accuracy_9: 0.2259 - dense_4_categorical_accuracy_10: 0.095 - ETA: 11s - loss: 16.5514 - dense_4_loss_1: 1.2076 - dense_4_loss_2: 1.0063 - dense_4_loss_3: 1.7399 - dense_4_loss_4: 2.6960 - dense_4_loss_5: 0.7685 - dense_4_loss_6: 1.2586 - dense_4_loss_7: 2.6482 - dense_4_loss_8: 0.9251 - dense_4_loss_9: 1.7091 - dense_4_loss_10: 2.5922 - dense_4_categorical_accuracy_1: 0.5297 - dense_4_categorical_accuracy_2: 0.6360 - dense_4_categorical_accuracy_3: 0.2855 - dense_4_categorical_accuracy_4: 0.0764 - dense_4_categorical_accuracy_5: 0.9518 - dense_4_categorical_accuracy_6: 0.3331 - dense_4_categorical_accuracy_7: 0.0601 - dense_4_categorical_accuracy_8: 0.8994 - dense_4_categorical_accuracy_9: 0.2281 - dense_4_categorical_accuracy_10: 0.096 - ETA: 10s - loss: 16.4559 - dense_4_loss_1: 1.1934 - dense_4_loss_2: 0.9935 - dense_4_loss_3: 1.7311 - dense_4_loss_4: 2.6900 - dense_4_loss_5: 0.7576 - dense_4_loss_6: 1.2463 - dense_4_loss_7: 2.6419 - dense_4_loss_8: 0.9117 - dense_4_loss_9: 1.7037 - dense_4_loss_10: 2.5867 - dense_4_categorical_accuracy_1: 0.5363 - dense_4_categorical_accuracy_2: 0.6410 - dense_4_categorical_accuracy_3: 0.2869 - dense_4_categorical_accuracy_4: 0.0772 - dense_4_categorical_accuracy_5: 0.9525 - dense_4_categorical_accuracy_6: 0.3409 - dense_4_categorical_accuracy_7: 0.0612 - dense_4_categorical_accuracy_8: 0.9009 - dense_4_categorical_accuracy_9: 0.2301 - dense_4_categorical_accuracy_10: 0.097 - ETA: 10s - loss: 16.3637 - dense_4_loss_1: 1.1794 - dense_4_loss_2: 0.9810 - dense_4_loss_3: 1.7216 - dense_4_loss_4: 2.6854 - dense_4_loss_5: 0.7468 - dense_4_loss_6: 1.2342 - dense_4_loss_7: 2.6351 - dense_4_loss_8: 0.8987 - dense_4_loss_9: 1.6983 - dense_4_loss_10: 2.5831 - dense_4_categorical_accuracy_1: 0.5426 - dense_4_categorical_accuracy_2: 0.6458 - dense_4_categorical_accuracy_3: 0.2890 - dense_4_categorical_accuracy_4: 0.0772 - dense_4_categorical_accuracy_5: 0.9532 - dense_4_categorical_accuracy_6: 0.3475 - dense_4_categorical_accuracy_7: 0.0633 - dense_4_categorical_accuracy_8: 0.9023 - dense_4_categorical_accuracy_9: 0.2313 - dense_4_categorical_accuracy_10: 0.098 - ETA: 10s - loss: 16.2717 - dense_4_loss_1: 1.1656 - dense_4_loss_2: 0.9690 - dense_4_loss_3: 1.7120 - dense_4_loss_4: 2.6796 - dense_4_loss_5: 0.7364 - dense_4_loss_6: 1.2225 - dense_4_loss_7: 2.6276 - dense_4_loss_8: 0.8861 - dense_4_loss_9: 1.6931 - dense_4_loss_10: 2.5798 - dense_4_categorical_accuracy_1: 0.5486 - dense_4_categorical_accuracy_2: 0.6503 - dense_4_categorical_accuracy_3: 0.2913 - dense_4_categorical_accuracy_4: 0.0786 - dense_4_categorical_accuracy_5: 0.9539 - dense_4_categorical_accuracy_6: 0.3540 - dense_4_categorical_accuracy_7: 0.0653 - dense_4_categorical_accuracy_8: 0.9037 - dense_4_categorical_accuracy_9: 0.2334 - dense_4_categorical_accuracy_10: 0.098 - ETA: 9s - loss: 16.1805 - dense_4_loss_1: 1.1524 - dense_4_loss_2: 0.9572 - dense_4_loss_3: 1.7026 - dense_4_loss_4: 2.6738 - dense_4_loss_5: 0.7262 - dense_4_loss_6: 1.2102 - dense_4_loss_7: 2.6200 - dense_4_loss_8: 0.8740 - dense_4_loss_9: 1.6880 - dense_4_loss_10: 2.5761 - dense_4_categorical_accuracy_1: 0.5545 - dense_4_categorical_accuracy_2: 0.6548 - dense_4_categorical_accuracy_3: 0.2945 - dense_4_categorical_accuracy_4: 0.0803 - dense_4_categorical_accuracy_5: 0.9545 - dense_4_categorical_accuracy_6: 0.3608 - dense_4_categorical_accuracy_7: 0.0675 - dense_4_categorical_accuracy_8: 0.9051 - dense_4_categorical_accuracy_9: 0.2361 - dense_4_categorical_accuracy_10: 0.098 - ETA: 9s - loss: 16.0943 - dense_4_loss_1: 1.1397 - dense_4_loss_2: 0.9464 - dense_4_loss_3: 1.6942 - dense_4_loss_4: 2.6671 - dense_4_loss_5: 0.7164 - dense_4_loss_6: 1.1996 - dense_4_loss_7: 2.6114 - dense_4_loss_8: 0.8623 - dense_4_loss_9: 1.6839 - dense_4_loss_10: 2.5732 - dense_4_categorical_accuracy_1: 0.5599 - dense_4_categorical_accuracy_2: 0.6589 - dense_4_categorical_accuracy_3: 0.2965 - dense_4_categorical_accuracy_4: 0.0814 - dense_4_categorical_accuracy_5: 0.9551 - dense_4_categorical_accuracy_6: 0.3665 - dense_4_categorical_accuracy_7: 0.0703 - dense_4_categorical_accuracy_8: 0.9064 - dense_4_categorical_accuracy_9: 0.2369 - dense_4_categorical_accuracy_10: 0.0990"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8400/10000 [========================>.....] - ETA: 8s - loss: 16.0109 - dense_4_loss_1: 1.1276 - dense_4_loss_2: 0.9362 - dense_4_loss_3: 1.6854 - dense_4_loss_4: 2.6623 - dense_4_loss_5: 0.7068 - dense_4_loss_6: 1.1894 - dense_4_loss_7: 2.6046 - dense_4_loss_8: 0.8510 - dense_4_loss_9: 1.6775 - dense_4_loss_10: 2.5700 - dense_4_categorical_accuracy_1: 0.5648 - dense_4_categorical_accuracy_2: 0.6625 - dense_4_categorical_accuracy_3: 0.3007 - dense_4_categorical_accuracy_4: 0.0823 - dense_4_categorical_accuracy_5: 0.9558 - dense_4_categorical_accuracy_6: 0.3727 - dense_4_categorical_accuracy_7: 0.0716 - dense_4_categorical_accuracy_8: 0.9077 - dense_4_categorical_accuracy_9: 0.2389 - dense_4_categorical_accuracy_10: 0.09 - ETA: 8s - loss: 15.9294 - dense_4_loss_1: 1.1162 - dense_4_loss_2: 0.9268 - dense_4_loss_3: 1.6784 - dense_4_loss_4: 2.6565 - dense_4_loss_5: 0.6974 - dense_4_loss_6: 1.1789 - dense_4_loss_7: 2.5973 - dense_4_loss_8: 0.8399 - dense_4_loss_9: 1.6711 - dense_4_loss_10: 2.5667 - dense_4_categorical_accuracy_1: 0.5695 - dense_4_categorical_accuracy_2: 0.6658 - dense_4_categorical_accuracy_3: 0.3030 - dense_4_categorical_accuracy_4: 0.0836 - dense_4_categorical_accuracy_5: 0.9564 - dense_4_categorical_accuracy_6: 0.3784 - dense_4_categorical_accuracy_7: 0.0735 - dense_4_categorical_accuracy_8: 0.9089 - dense_4_categorical_accuracy_9: 0.2419 - dense_4_categorical_accuracy_10: 0.09 - ETA: 8s - loss: 15.8467 - dense_4_loss_1: 1.1038 - dense_4_loss_2: 0.9157 - dense_4_loss_3: 1.6697 - dense_4_loss_4: 2.6511 - dense_4_loss_5: 0.6883 - dense_4_loss_6: 1.1702 - dense_4_loss_7: 2.5907 - dense_4_loss_8: 0.8290 - dense_4_loss_9: 1.6659 - dense_4_loss_10: 2.5623 - dense_4_categorical_accuracy_1: 0.5751 - dense_4_categorical_accuracy_2: 0.6700 - dense_4_categorical_accuracy_3: 0.3057 - dense_4_categorical_accuracy_4: 0.0852 - dense_4_categorical_accuracy_5: 0.9569 - dense_4_categorical_accuracy_6: 0.3833 - dense_4_categorical_accuracy_7: 0.0765 - dense_4_categorical_accuracy_8: 0.9101 - dense_4_categorical_accuracy_9: 0.2436 - dense_4_categorical_accuracy_10: 0.10 - ETA: 7s - loss: 15.7652 - dense_4_loss_1: 1.0914 - dense_4_loss_2: 0.9048 - dense_4_loss_3: 1.6620 - dense_4_loss_4: 2.6460 - dense_4_loss_5: 0.6794 - dense_4_loss_6: 1.1594 - dense_4_loss_7: 2.5853 - dense_4_loss_8: 0.8183 - dense_4_loss_9: 1.6602 - dense_4_loss_10: 2.5585 - dense_4_categorical_accuracy_1: 0.5805 - dense_4_categorical_accuracy_2: 0.6742 - dense_4_categorical_accuracy_3: 0.3071 - dense_4_categorical_accuracy_4: 0.0857 - dense_4_categorical_accuracy_5: 0.9575 - dense_4_categorical_accuracy_6: 0.3896 - dense_4_categorical_accuracy_7: 0.0778 - dense_4_categorical_accuracy_8: 0.9113 - dense_4_categorical_accuracy_9: 0.2461 - dense_4_categorical_accuracy_10: 0.10 - ETA: 7s - loss: 15.6929 - dense_4_loss_1: 1.0812 - dense_4_loss_2: 0.8967 - dense_4_loss_3: 1.6546 - dense_4_loss_4: 2.6412 - dense_4_loss_5: 0.6707 - dense_4_loss_6: 1.1503 - dense_4_loss_7: 2.5790 - dense_4_loss_8: 0.8079 - dense_4_loss_9: 1.6562 - dense_4_loss_10: 2.5551 - dense_4_categorical_accuracy_1: 0.5844 - dense_4_categorical_accuracy_2: 0.6769 - dense_4_categorical_accuracy_3: 0.3097 - dense_4_categorical_accuracy_4: 0.0858 - dense_4_categorical_accuracy_5: 0.9581 - dense_4_categorical_accuracy_6: 0.3956 - dense_4_categorical_accuracy_7: 0.0787 - dense_4_categorical_accuracy_8: 0.9125 - dense_4_categorical_accuracy_9: 0.2474 - dense_4_categorical_accuracy_10: 0.10 - ETA: 7s - loss: 15.6138 - dense_4_loss_1: 1.0696 - dense_4_loss_2: 0.8869 - dense_4_loss_3: 1.6471 - dense_4_loss_4: 2.6356 - dense_4_loss_5: 0.6623 - dense_4_loss_6: 1.1410 - dense_4_loss_7: 2.5710 - dense_4_loss_8: 0.7977 - dense_4_loss_9: 1.6503 - dense_4_loss_10: 2.5522 - dense_4_categorical_accuracy_1: 0.5892 - dense_4_categorical_accuracy_2: 0.6805 - dense_4_categorical_accuracy_3: 0.3121 - dense_4_categorical_accuracy_4: 0.0871 - dense_4_categorical_accuracy_5: 0.9586 - dense_4_categorical_accuracy_6: 0.4018 - dense_4_categorical_accuracy_7: 0.0812 - dense_4_categorical_accuracy_8: 0.9136 - dense_4_categorical_accuracy_9: 0.2491 - dense_4_categorical_accuracy_10: 0.10 - ETA: 6s - loss: 15.5355 - dense_4_loss_1: 1.0576 - dense_4_loss_2: 0.8765 - dense_4_loss_3: 1.6393 - dense_4_loss_4: 2.6308 - dense_4_loss_5: 0.6540 - dense_4_loss_6: 1.1311 - dense_4_loss_7: 2.5644 - dense_4_loss_8: 0.7878 - dense_4_loss_9: 1.6459 - dense_4_loss_10: 2.5480 - dense_4_categorical_accuracy_1: 0.5944 - dense_4_categorical_accuracy_2: 0.6844 - dense_4_categorical_accuracy_3: 0.3141 - dense_4_categorical_accuracy_4: 0.0881 - dense_4_categorical_accuracy_5: 0.9591 - dense_4_categorical_accuracy_6: 0.4077 - dense_4_categorical_accuracy_7: 0.0834 - dense_4_categorical_accuracy_8: 0.9147 - dense_4_categorical_accuracy_9: 0.2499 - dense_4_categorical_accuracy_10: 0.10 - ETA: 6s - loss: 15.4610 - dense_4_loss_1: 1.0463 - dense_4_loss_2: 0.8667 - dense_4_loss_3: 1.6315 - dense_4_loss_4: 2.6257 - dense_4_loss_5: 0.6460 - dense_4_loss_6: 1.1232 - dense_4_loss_7: 2.5578 - dense_4_loss_8: 0.7780 - dense_4_loss_9: 1.6411 - dense_4_loss_10: 2.5448 - dense_4_categorical_accuracy_1: 0.5993 - dense_4_categorical_accuracy_2: 0.6883 - dense_4_categorical_accuracy_3: 0.3165 - dense_4_categorical_accuracy_4: 0.0889 - dense_4_categorical_accuracy_5: 0.9596 - dense_4_categorical_accuracy_6: 0.4119 - dense_4_categorical_accuracy_7: 0.0864 - dense_4_categorical_accuracy_8: 0.9158 - dense_4_categorical_accuracy_9: 0.2515 - dense_4_categorical_accuracy_10: 0.10 - ETA: 6s - loss: 15.3902 - dense_4_loss_1: 1.0355 - dense_4_loss_2: 0.8575 - dense_4_loss_3: 1.6237 - dense_4_loss_4: 2.6209 - dense_4_loss_5: 0.6381 - dense_4_loss_6: 1.1135 - dense_4_loss_7: 2.5526 - dense_4_loss_8: 0.7685 - dense_4_loss_9: 1.6377 - dense_4_loss_10: 2.5420 - dense_4_categorical_accuracy_1: 0.6036 - dense_4_categorical_accuracy_2: 0.6915 - dense_4_categorical_accuracy_3: 0.3193 - dense_4_categorical_accuracy_4: 0.0902 - dense_4_categorical_accuracy_5: 0.9601 - dense_4_categorical_accuracy_6: 0.4172 - dense_4_categorical_accuracy_7: 0.0878 - dense_4_categorical_accuracy_8: 0.9168 - dense_4_categorical_accuracy_9: 0.2519 - dense_4_categorical_accuracy_10: 0.10 - ETA: 5s - loss: 15.3169 - dense_4_loss_1: 1.0245 - dense_4_loss_2: 0.8482 - dense_4_loss_3: 1.6157 - dense_4_loss_4: 2.6154 - dense_4_loss_5: 0.6305 - dense_4_loss_6: 1.1051 - dense_4_loss_7: 2.5461 - dense_4_loss_8: 0.7592 - dense_4_loss_9: 1.6330 - dense_4_loss_10: 2.5391 - dense_4_categorical_accuracy_1: 0.6082 - dense_4_categorical_accuracy_2: 0.6950 - dense_4_categorical_accuracy_3: 0.3230 - dense_4_categorical_accuracy_4: 0.0916 - dense_4_categorical_accuracy_5: 0.9606 - dense_4_categorical_accuracy_6: 0.4218 - dense_4_categorical_accuracy_7: 0.0911 - dense_4_categorical_accuracy_8: 0.9178 - dense_4_categorical_accuracy_9: 0.2530 - dense_4_categorical_accuracy_10: 0.10 - ETA: 5s - loss: 15.2459 - dense_4_loss_1: 1.0143 - dense_4_loss_2: 0.8394 - dense_4_loss_3: 1.6080 - dense_4_loss_4: 2.6104 - dense_4_loss_5: 0.6230 - dense_4_loss_6: 1.0967 - dense_4_loss_7: 2.5406 - dense_4_loss_8: 0.7502 - dense_4_loss_9: 1.6286 - dense_4_loss_10: 2.5346 - dense_4_categorical_accuracy_1: 0.6123 - dense_4_categorical_accuracy_2: 0.6982 - dense_4_categorical_accuracy_3: 0.3255 - dense_4_categorical_accuracy_4: 0.0925 - dense_4_categorical_accuracy_5: 0.9611 - dense_4_categorical_accuracy_6: 0.4271 - dense_4_categorical_accuracy_7: 0.0919 - dense_4_categorical_accuracy_8: 0.9188 - dense_4_categorical_accuracy_9: 0.2549 - dense_4_categorical_accuracy_10: 0.10 - ETA: 4s - loss: 15.1776 - dense_4_loss_1: 1.0042 - dense_4_loss_2: 0.8308 - dense_4_loss_3: 1.6017 - dense_4_loss_4: 2.6057 - dense_4_loss_5: 0.6157 - dense_4_loss_6: 1.0880 - dense_4_loss_7: 2.5335 - dense_4_loss_8: 0.7414 - dense_4_loss_9: 1.6251 - dense_4_loss_10: 2.5315 - dense_4_categorical_accuracy_1: 0.6164 - dense_4_categorical_accuracy_2: 0.7014 - dense_4_categorical_accuracy_3: 0.3269 - dense_4_categorical_accuracy_4: 0.0938 - dense_4_categorical_accuracy_5: 0.9615 - dense_4_categorical_accuracy_6: 0.4329 - dense_4_categorical_accuracy_7: 0.0939 - dense_4_categorical_accuracy_8: 0.9198 - dense_4_categorical_accuracy_9: 0.2562 - dense_4_categorical_accuracy_10: 0.1054"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9600/10000 [===========================>..] - ETA: 4s - loss: 15.1129 - dense_4_loss_1: 0.9945 - dense_4_loss_2: 0.8226 - dense_4_loss_3: 1.5947 - dense_4_loss_4: 2.6014 - dense_4_loss_5: 0.6086 - dense_4_loss_6: 1.0797 - dense_4_loss_7: 2.5282 - dense_4_loss_8: 0.7328 - dense_4_loss_9: 1.6215 - dense_4_loss_10: 2.5288 - dense_4_categorical_accuracy_1: 0.6205 - dense_4_categorical_accuracy_2: 0.7045 - dense_4_categorical_accuracy_3: 0.3293 - dense_4_categorical_accuracy_4: 0.0944 - dense_4_categorical_accuracy_5: 0.9620 - dense_4_categorical_accuracy_6: 0.4380 - dense_4_categorical_accuracy_7: 0.0951 - dense_4_categorical_accuracy_8: 0.9207 - dense_4_categorical_accuracy_9: 0.2580 - dense_4_categorical_accuracy_10: 0.10 - ETA: 4s - loss: 15.0481 - dense_4_loss_1: 0.9851 - dense_4_loss_2: 0.8147 - dense_4_loss_3: 1.5882 - dense_4_loss_4: 2.5970 - dense_4_loss_5: 0.6017 - dense_4_loss_6: 1.0716 - dense_4_loss_7: 2.5224 - dense_4_loss_8: 0.7244 - dense_4_loss_9: 1.6175 - dense_4_loss_10: 2.5255 - dense_4_categorical_accuracy_1: 0.6242 - dense_4_categorical_accuracy_2: 0.7072 - dense_4_categorical_accuracy_3: 0.3303 - dense_4_categorical_accuracy_4: 0.0950 - dense_4_categorical_accuracy_5: 0.9624 - dense_4_categorical_accuracy_6: 0.4429 - dense_4_categorical_accuracy_7: 0.0969 - dense_4_categorical_accuracy_8: 0.9216 - dense_4_categorical_accuracy_9: 0.2594 - dense_4_categorical_accuracy_10: 0.10 - ETA: 4s - loss: 14.9819 - dense_4_loss_1: 0.9750 - dense_4_loss_2: 0.8060 - dense_4_loss_3: 1.5810 - dense_4_loss_4: 2.5925 - dense_4_loss_5: 0.5950 - dense_4_loss_6: 1.0636 - dense_4_loss_7: 2.5165 - dense_4_loss_8: 0.7162 - dense_4_loss_9: 1.6127 - dense_4_loss_10: 2.5235 - dense_4_categorical_accuracy_1: 0.6283 - dense_4_categorical_accuracy_2: 0.7105 - dense_4_categorical_accuracy_3: 0.3323 - dense_4_categorical_accuracy_4: 0.0959 - dense_4_categorical_accuracy_5: 0.9629 - dense_4_categorical_accuracy_6: 0.4478 - dense_4_categorical_accuracy_7: 0.0991 - dense_4_categorical_accuracy_8: 0.9225 - dense_4_categorical_accuracy_9: 0.2622 - dense_4_categorical_accuracy_10: 0.10 - ETA: 3s - loss: 14.9146 - dense_4_loss_1: 0.9650 - dense_4_loss_2: 0.7977 - dense_4_loss_3: 1.5744 - dense_4_loss_4: 2.5880 - dense_4_loss_5: 0.5884 - dense_4_loss_6: 1.0549 - dense_4_loss_7: 2.5108 - dense_4_loss_8: 0.7082 - dense_4_loss_9: 1.6078 - dense_4_loss_10: 2.5194 - dense_4_categorical_accuracy_1: 0.6324 - dense_4_categorical_accuracy_2: 0.7135 - dense_4_categorical_accuracy_3: 0.3343 - dense_4_categorical_accuracy_4: 0.0973 - dense_4_categorical_accuracy_5: 0.9633 - dense_4_categorical_accuracy_6: 0.4531 - dense_4_categorical_accuracy_7: 0.1000 - dense_4_categorical_accuracy_8: 0.9234 - dense_4_categorical_accuracy_9: 0.2642 - dense_4_categorical_accuracy_10: 0.10 - ETA: 3s - loss: 14.8560 - dense_4_loss_1: 0.9554 - dense_4_loss_2: 0.7899 - dense_4_loss_3: 1.5687 - dense_4_loss_4: 2.5838 - dense_4_loss_5: 0.5819 - dense_4_loss_6: 1.0488 - dense_4_loss_7: 2.5053 - dense_4_loss_8: 0.7003 - dense_4_loss_9: 1.6055 - dense_4_loss_10: 2.5163 - dense_4_categorical_accuracy_1: 0.6363 - dense_4_categorical_accuracy_2: 0.7163 - dense_4_categorical_accuracy_3: 0.3352 - dense_4_categorical_accuracy_4: 0.0982 - dense_4_categorical_accuracy_5: 0.9637 - dense_4_categorical_accuracy_6: 0.4562 - dense_4_categorical_accuracy_7: 0.1018 - dense_4_categorical_accuracy_8: 0.9243 - dense_4_categorical_accuracy_9: 0.2637 - dense_4_categorical_accuracy_10: 0.10 - ETA: 3s - loss: 14.7909 - dense_4_loss_1: 0.9461 - dense_4_loss_2: 0.7821 - dense_4_loss_3: 1.5615 - dense_4_loss_4: 2.5796 - dense_4_loss_5: 0.5755 - dense_4_loss_6: 1.0406 - dense_4_loss_7: 2.4987 - dense_4_loss_8: 0.6926 - dense_4_loss_9: 1.6006 - dense_4_loss_10: 2.5137 - dense_4_categorical_accuracy_1: 0.6401 - dense_4_categorical_accuracy_2: 0.7192 - dense_4_categorical_accuracy_3: 0.3383 - dense_4_categorical_accuracy_4: 0.0988 - dense_4_categorical_accuracy_5: 0.9641 - dense_4_categorical_accuracy_6: 0.4609 - dense_4_categorical_accuracy_7: 0.1044 - dense_4_categorical_accuracy_8: 0.9251 - dense_4_categorical_accuracy_9: 0.2653 - dense_4_categorical_accuracy_10: 0.10 - ETA: 2s - loss: 14.7251 - dense_4_loss_1: 0.9364 - dense_4_loss_2: 0.7739 - dense_4_loss_3: 1.5553 - dense_4_loss_4: 2.5737 - dense_4_loss_5: 0.5693 - dense_4_loss_6: 1.0327 - dense_4_loss_7: 2.4920 - dense_4_loss_8: 0.6851 - dense_4_loss_9: 1.5959 - dense_4_loss_10: 2.5107 - dense_4_categorical_accuracy_1: 0.6440 - dense_4_categorical_accuracy_2: 0.7223 - dense_4_categorical_accuracy_3: 0.3399 - dense_4_categorical_accuracy_4: 0.1005 - dense_4_categorical_accuracy_5: 0.9645 - dense_4_categorical_accuracy_6: 0.4655 - dense_4_categorical_accuracy_7: 0.1066 - dense_4_categorical_accuracy_8: 0.9259 - dense_4_categorical_accuracy_9: 0.2673 - dense_4_categorical_accuracy_10: 0.11 - ETA: 2s - loss: 14.6664 - dense_4_loss_1: 0.9277 - dense_4_loss_2: 0.7670 - dense_4_loss_3: 1.5492 - dense_4_loss_4: 2.5700 - dense_4_loss_5: 0.5632 - dense_4_loss_6: 1.0257 - dense_4_loss_7: 2.4870 - dense_4_loss_8: 0.6777 - dense_4_loss_9: 1.5915 - dense_4_loss_10: 2.5073 - dense_4_categorical_accuracy_1: 0.6474 - dense_4_categorical_accuracy_2: 0.7249 - dense_4_categorical_accuracy_3: 0.3420 - dense_4_categorical_accuracy_4: 0.1009 - dense_4_categorical_accuracy_5: 0.9649 - dense_4_categorical_accuracy_6: 0.4698 - dense_4_categorical_accuracy_7: 0.1071 - dense_4_categorical_accuracy_8: 0.9267 - dense_4_categorical_accuracy_9: 0.2689 - dense_4_categorical_accuracy_10: 0.11 - ETA: 2s - loss: 14.6046 - dense_4_loss_1: 0.9186 - dense_4_loss_2: 0.7594 - dense_4_loss_3: 1.5436 - dense_4_loss_4: 2.5647 - dense_4_loss_5: 0.5573 - dense_4_loss_6: 1.0180 - dense_4_loss_7: 2.4801 - dense_4_loss_8: 0.6705 - dense_4_loss_9: 1.5877 - dense_4_loss_10: 2.5047 - dense_4_categorical_accuracy_1: 0.6510 - dense_4_categorical_accuracy_2: 0.7276 - dense_4_categorical_accuracy_3: 0.3434 - dense_4_categorical_accuracy_4: 0.1024 - dense_4_categorical_accuracy_5: 0.9653 - dense_4_categorical_accuracy_6: 0.4742 - dense_4_categorical_accuracy_7: 0.1092 - dense_4_categorical_accuracy_8: 0.9275 - dense_4_categorical_accuracy_9: 0.2701 - dense_4_categorical_accuracy_10: 0.11 - ETA: 1s - loss: 14.5445 - dense_4_loss_1: 0.9098 - dense_4_loss_2: 0.7520 - dense_4_loss_3: 1.5366 - dense_4_loss_4: 2.5597 - dense_4_loss_5: 0.5514 - dense_4_loss_6: 1.0115 - dense_4_loss_7: 2.4753 - dense_4_loss_8: 0.6634 - dense_4_loss_9: 1.5831 - dense_4_loss_10: 2.5016 - dense_4_categorical_accuracy_1: 0.6544 - dense_4_categorical_accuracy_2: 0.7304 - dense_4_categorical_accuracy_3: 0.3466 - dense_4_categorical_accuracy_4: 0.1045 - dense_4_categorical_accuracy_5: 0.9656 - dense_4_categorical_accuracy_6: 0.4779 - dense_4_categorical_accuracy_7: 0.1110 - dense_4_categorical_accuracy_8: 0.9283 - dense_4_categorical_accuracy_9: 0.2714 - dense_4_categorical_accuracy_10: 0.11 - ETA: 1s - loss: 14.4845 - dense_4_loss_1: 0.9010 - dense_4_loss_2: 0.7447 - dense_4_loss_3: 1.5294 - dense_4_loss_4: 2.5556 - dense_4_loss_5: 0.5457 - dense_4_loss_6: 1.0043 - dense_4_loss_7: 2.4695 - dense_4_loss_8: 0.6566 - dense_4_loss_9: 1.5790 - dense_4_loss_10: 2.4989 - dense_4_categorical_accuracy_1: 0.6579 - dense_4_categorical_accuracy_2: 0.7332 - dense_4_categorical_accuracy_3: 0.3497 - dense_4_categorical_accuracy_4: 0.1052 - dense_4_categorical_accuracy_5: 0.9660 - dense_4_categorical_accuracy_6: 0.4815 - dense_4_categorical_accuracy_7: 0.1124 - dense_4_categorical_accuracy_8: 0.9291 - dense_4_categorical_accuracy_9: 0.2734 - dense_4_categorical_accuracy_10: 0.11 - ETA: 1s - loss: 14.4288 - dense_4_loss_1: 0.8936 - dense_4_loss_2: 0.7385 - dense_4_loss_3: 1.5236 - dense_4_loss_4: 2.5511 - dense_4_loss_5: 0.5401 - dense_4_loss_6: 0.9967 - dense_4_loss_7: 2.4649 - dense_4_loss_8: 0.6498 - dense_4_loss_9: 1.5743 - dense_4_loss_10: 2.4963 - dense_4_categorical_accuracy_1: 0.6608 - dense_4_categorical_accuracy_2: 0.7353 - dense_4_categorical_accuracy_3: 0.3523 - dense_4_categorical_accuracy_4: 0.1058 - dense_4_categorical_accuracy_5: 0.9664 - dense_4_categorical_accuracy_6: 0.4855 - dense_4_categorical_accuracy_7: 0.1141 - dense_4_categorical_accuracy_8: 0.9298 - dense_4_categorical_accuracy_9: 0.2751 - dense_4_categorical_accuracy_10: 0.1136"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - ETA: 0s - loss: 14.3713 - dense_4_loss_1: 0.8856 - dense_4_loss_2: 0.7320 - dense_4_loss_3: 1.5175 - dense_4_loss_4: 2.5468 - dense_4_loss_5: 0.5347 - dense_4_loss_6: 0.9899 - dense_4_loss_7: 2.4575 - dense_4_loss_8: 0.6432 - dense_4_loss_9: 1.5709 - dense_4_loss_10: 2.4933 - dense_4_categorical_accuracy_1: 0.6640 - dense_4_categorical_accuracy_2: 0.7377 - dense_4_categorical_accuracy_3: 0.3545 - dense_4_categorical_accuracy_4: 0.1067 - dense_4_categorical_accuracy_5: 0.9667 - dense_4_categorical_accuracy_6: 0.4893 - dense_4_categorical_accuracy_7: 0.1171 - dense_4_categorical_accuracy_8: 0.9305 - dense_4_categorical_accuracy_9: 0.2768 - dense_4_categorical_accuracy_10: 0.11 - ETA: 0s - loss: 14.3154 - dense_4_loss_1: 0.8772 - dense_4_loss_2: 0.7250 - dense_4_loss_3: 1.5110 - dense_4_loss_4: 2.5421 - dense_4_loss_5: 0.5294 - dense_4_loss_6: 0.9831 - dense_4_loss_7: 2.4520 - dense_4_loss_8: 0.6367 - dense_4_loss_9: 1.5681 - dense_4_loss_10: 2.4907 - dense_4_categorical_accuracy_1: 0.6674 - dense_4_categorical_accuracy_2: 0.7402 - dense_4_categorical_accuracy_3: 0.3570 - dense_4_categorical_accuracy_4: 0.1074 - dense_4_categorical_accuracy_5: 0.9670 - dense_4_categorical_accuracy_6: 0.4931 - dense_4_categorical_accuracy_7: 0.1187 - dense_4_categorical_accuracy_8: 0.9312 - dense_4_categorical_accuracy_9: 0.2778 - dense_4_categorical_accuracy_10: 0.11 - ETA: 0s - loss: 14.2608 - dense_4_loss_1: 0.8694 - dense_4_loss_2: 0.7186 - dense_4_loss_3: 1.5054 - dense_4_loss_4: 2.5379 - dense_4_loss_5: 0.5242 - dense_4_loss_6: 0.9772 - dense_4_loss_7: 2.4464 - dense_4_loss_8: 0.6304 - dense_4_loss_9: 1.5640 - dense_4_loss_10: 2.4873 - dense_4_categorical_accuracy_1: 0.6705 - dense_4_categorical_accuracy_2: 0.7425 - dense_4_categorical_accuracy_3: 0.3588 - dense_4_categorical_accuracy_4: 0.1084 - dense_4_categorical_accuracy_5: 0.9674 - dense_4_categorical_accuracy_6: 0.4965 - dense_4_categorical_accuracy_7: 0.1202 - dense_4_categorical_accuracy_8: 0.9319 - dense_4_categorical_accuracy_9: 0.2803 - dense_4_categorical_accuracy_10: 0.11 - 29s 3ms/step - loss: 14.2065 - dense_4_loss_1: 0.8617 - dense_4_loss_2: 0.7122 - dense_4_loss_3: 1.4994 - dense_4_loss_4: 2.5340 - dense_4_loss_5: 0.5191 - dense_4_loss_6: 0.9698 - dense_4_loss_7: 2.4405 - dense_4_loss_8: 0.6241 - dense_4_loss_9: 1.5602 - dense_4_loss_10: 2.4855 - dense_4_categorical_accuracy_1: 0.6737 - dense_4_categorical_accuracy_2: 0.7448 - dense_4_categorical_accuracy_3: 0.3605 - dense_4_categorical_accuracy_4: 0.1089 - dense_4_categorical_accuracy_5: 0.9677 - dense_4_categorical_accuracy_6: 0.5009 - dense_4_categorical_accuracy_7: 0.1217 - dense_4_categorical_accuracy_8: 0.9326 - dense_4_categorical_accuracy_9: 0.2816 - dense_4_categorical_accuracy_10: 0.1176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20524878d68>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training you can see the loss as well as the accuracy on each of the 10 positions of the output. The table below gives you an example of what the accuracies could be if the batch had 2 examples: \n",
    "\n",
    "<img src=\"images/table.png\" style=\"width:700;height:200px;\"> <br>\n",
    "<caption><center>Thus, `dense_2_acc_8: 0.89` means that you are predicting the 7th character of the output correctly 89% of the time in the current batch of data. </center></caption>\n",
    "\n",
    "\n",
    "We have run this model for longer, and saved the weights. Run the next cell to load our weights. (By training a model for several minutes, you should be able to obtain a model of similar accuracy, but loading our model will save you time.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the results on new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = EXAMPLES[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = string_to_int(example, Tx, human_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "source1 = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 30)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03\n",
      "source: 5 April 09\n",
      "output: 2009-03-05\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-01\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09\n",
      "source: March 3 2001\n",
      "output: 2001-03-03\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "#this line is incorrect\n",
    "#source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "#should change to these 2 lines:\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "    source = np.expand_dims(source,0)\n",
    "#should change to these 2 lines:\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also change these examples to test with your own examples. The next part will give you a better sense on what the attention mechanism is doing--i.e., what part of the input the network is paying attention to when generating a particular output character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Visualizing Attention (Optional / Ungraded)\n",
    "\n",
    "Since the problem has a fixed output length of 10, it is also possible to carry out this task using 10 different softmax units to generate the 10 characters of the output. But one advantage of the attention model is that each part of the output (say the month) knows it needs to depend only on a small part of the input (the characters in the input giving the month). We can  visualize what part of the output is looking at what part of the input.\n",
    "\n",
    "Consider the task of translating \"Saturday 9 May 2018\" to \"2018-05-09\". If we visualize the computed $\\alpha^{\\langle t, t' \\rangle}$ we get this: \n",
    "\n",
    "<img src=\"images/date_attention.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 8**: Full Attention Map</center></caption>\n",
    "\n",
    "Notice how the output ignores the \"Saturday\" portion of the input. None of the output timesteps are paying much attention to that portion of the input. We see also that 9 has been translated as 09 and May has been correctly translated into 05, with the output paying attention to the parts of the input it needs to to make the translation. The year mostly requires it to pay attention to the input's \"18\" in order to generate \"2018.\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Getting the activations from the network\n",
    "\n",
    "Lets now visualize the attention values in your network. We'll propagate an example through the network, then visualize the values of $\\alpha^{\\langle t, t' \\rangle}$. \n",
    "\n",
    "To figure out where the attention values are located, let's start by printing a summary of the model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "s0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 30, 128)      0           s0[0][0]                         \n",
      "                                                                 lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 lstm_2[2][0]                     \n",
      "                                                                 lstm_2[3][0]                     \n",
      "                                                                 lstm_2[4][0]                     \n",
      "                                                                 lstm_2[5][0]                     \n",
      "                                                                 lstm_2[6][0]                     \n",
      "                                                                 lstm_2[7][0]                     \n",
      "                                                                 lstm_2[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 128)      52224       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 30, 256)      0           repeat_vector_2[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[9][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 30, 1)        257         concatenate_2[0][0]              \n",
      "                                                                 concatenate_2[1][0]              \n",
      "                                                                 concatenate_2[2][0]              \n",
      "                                                                 concatenate_2[3][0]              \n",
      "                                                                 concatenate_2[4][0]              \n",
      "                                                                 concatenate_2[5][0]              \n",
      "                                                                 concatenate_2[6][0]              \n",
      "                                                                 concatenate_2[7][0]              \n",
      "                                                                 concatenate_2[8][0]              \n",
      "                                                                 concatenate_2[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_3[0][0]                    \n",
      "                                                                 dense_3[1][0]                    \n",
      "                                                                 dense_3[2][0]                    \n",
      "                                                                 dense_3[3][0]                    \n",
      "                                                                 dense_3[4][0]                    \n",
      "                                                                 dense_3[5][0]                    \n",
      "                                                                 dense_3[6][0]                    \n",
      "                                                                 dense_3[7][0]                    \n",
      "                                                                 dense_3[8][0]                    \n",
      "                                                                 dense_3[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 128), (None, 131584      dot_2[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_2[1][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "                                                                 dot_2[2][0]                      \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 lstm_2[1][2]                     \n",
      "                                                                 dot_2[3][0]                      \n",
      "                                                                 lstm_2[2][0]                     \n",
      "                                                                 lstm_2[2][2]                     \n",
      "                                                                 dot_2[4][0]                      \n",
      "                                                                 lstm_2[3][0]                     \n",
      "                                                                 lstm_2[3][2]                     \n",
      "                                                                 dot_2[5][0]                      \n",
      "                                                                 lstm_2[4][0]                     \n",
      "                                                                 lstm_2[4][2]                     \n",
      "                                                                 dot_2[6][0]                      \n",
      "                                                                 lstm_2[5][0]                     \n",
      "                                                                 lstm_2[5][2]                     \n",
      "                                                                 dot_2[7][0]                      \n",
      "                                                                 lstm_2[6][0]                     \n",
      "                                                                 lstm_2[6][2]                     \n",
      "                                                                 dot_2[8][0]                      \n",
      "                                                                 lstm_2[7][0]                     \n",
      "                                                                 lstm_2[7][2]                     \n",
      "                                                                 dot_2[9][0]                      \n",
      "                                                                 lstm_2[8][0]                     \n",
      "                                                                 lstm_2[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 11)           1419        lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "                                                                 lstm_2[2][0]                     \n",
      "                                                                 lstm_2[3][0]                     \n",
      "                                                                 lstm_2[4][0]                     \n",
      "                                                                 lstm_2[5][0]                     \n",
      "                                                                 lstm_2[6][0]                     \n",
      "                                                                 lstm_2[7][0]                     \n",
      "                                                                 lstm_2[8][0]                     \n",
      "                                                                 lstm_2[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 185,484\n",
      "Trainable params: 185,484\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate through the output of `model.summary()` above. You can see that the layer named `attention_weights` outputs the `alphas` of shape (m, 30, 1) before `dot_2` computes the context vector for every time step $t = 0, \\ldots, T_y-1$. Lets get the activations from this layer.\n",
    "\n",
    "The function `attention_map()` pulls out the attention values from your model and plots them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2053b0b9630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAGsCAYAAAD9ro91AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZJazmQtSVLLmawlSWo5k7UkSS33/wGMFu5gkJXLEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x205376d2748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday April 08 1993\", num = 6, n_s = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the generated plot you can observe the values of the attention weights for each character of the predicted output. Examine this plot and check that where the network is paying attention makes sense to you.\n",
    "\n",
    "In the date translation application, you will observe that most of the time attention helps predict the year, and hasn't much impact on predicting the day/month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "\n",
    "You have come to the end of this assignment \n",
    "\n",
    "<font color='blue'> **Here's what you should remember from this notebook**:\n",
    "\n",
    "- Machine translation models can be used to map from one sequence to another. They are useful not just for translating human languages (like French->English) but also for tasks like date format translation. \n",
    "- An attention mechanism allows a network to focus on the most relevant parts of the input when producing a specific part of the output. \n",
    "- A network using an attention mechanism can translate from inputs of length $T_x$ to outputs of length $T_y$, where $T_x$ and $T_y$ can be different. \n",
    "- You can visualize attention weights $\\alpha^{\\langle t,t' \\rangle}$ to see what the network is paying attention to while generating each output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing this assignment! You are now able to implement an attention model and use it to learn complex mappings from one sequence to another. "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
